2024-12-08 18:00:41,812 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-08 18:00:41,819 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-08 18:00:41,868 - llm_processor - INFO - Models directory: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 18:00:41,868 - llm_processor - INFO - Loading model pipeline: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 18:00:42,345 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-12-08 18:00:44,782 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
2024-12-08 18:00:44,783 - llm_processor - INFO - Model pipeline loaded: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 18:00:44,784 - __main__ - INFO - Initializing knowledge base for detection
2024-12-08 18:00:44,784 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-08 18:00:44,784 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-08 18:00:44,785 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-08 18:00:44,786 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-08 18:16:26,200 - __main__ - INFO - Received task message
2024-12-08 18:16:26,201 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-08 18:16:26,201 - __main__ - INFO - Task type: detection
2024-12-08 18:16:26,203 - __main__ - INFO - Task data: {'Data_Processing.py': '\nclass OrderProcessor:\n    def __init__(self, validator, payment_gateway, notification_service):\n        self.validator = validator\n        self.payment_gateway = payment_gateway\n        self.notification_service = notification_service\n\n    def process_order(self, order):\n        if not self.validator.validate(order):\n            raise ValueError("Order is invalid")\n\n        payment_status = self.payment_gateway.charge(order)\n        if payment_status == "success":\n            self.notification_service.send_confirmation(order)\n        else:\n            self.notification_service.send_failure(order)\n    ', 'Data_Cleaning.py': '\nclass GodClass:\n    def __init__(self, config_path: str, db_path: str):\n        # Configuration\n        self.config = self._load_config(config_path)\n\n        # Database connection\n        self.db_path = db_path\n        self.conn = None\n\n        # Data cache\n        self.data_cache = []\n\n        # Logging settings\n        self.log_file = self.config.get("log_file", "app.log")\n        self.log_level = self.config.get("log_level", "INFO")\n\n        # State\n        self.user_is_logged_in = False\n        self.current_user = None\n\n    def _load_config(self, config_path: str):\n        # Load configuration from a JSON file\n        try:\n            with open(config_path, "r") as f:\n                config = json.load(f)\n        except FileNotFoundError:\n            config = {\n                "log_file": "app.log",\n                "log_level": "INFO",\n                "default_admin": "admin",\n                "default_password": "admin123"\n            }\n        return config\n\n    def log(self, message: str, level: str = "INFO"):\n        # Log a message to a file, ignoring log_level thresholds for simplicity\n        with open(self.log_file, "a") as f:\n            f.write(f"[{level}] {message}\n")\n\n    def connect_db(self):\n        # Connect to the database\n        if self.conn is None:\n            self.conn = sqlite3.connect(self.db_path)\n            self.log("Connected to the database.", "DEBUG")\n\n    def create_tables(self):\n        # Create tables if they don\'t exist\n        self.connect_db()\n        cursor = self.conn.cursor()\n        self.conn.commit()\n        self.log("Tables ensured in database.", "DEBUG")\n\n    def add_user(self, username: str, password: str):\n        # Add a user to the database\n        self.connect_db()\n        try:\n            cursor = self.conn.cursor()\n            cursor.execute("INSERT INTO users (username, password) VALUES (?, ?)", (username, password))\n            self.conn.commit()\n            self.log(f"User added: {username}", "INFO")\n        except sqlite3.IntegrityError:\n            self.log(f"User {username} already exists.", "ERROR")\n\n    def login_user(self, username: str, password: str):\n        # Login a user\n        self.connect_db()\n        cursor = self.conn.cursor()\n        cursor.execute("SELECT * FROM users WHERE username = ? AND password = ?", (username, password))\n        user = cursor.fetchone()\n        if user:\n            self.user_is_logged_in = True\n            self.current_user = username\n            self.log(f"User {username} logged in.", "INFO")\n            return True\n        else:\n            self.log(f"Failed login attempt for user {username}.", "WARNING")\n            return False\n\n    def add_item(self, name: str, value: int):\n        # Add an item to the database\n        if not self.user_is_logged_in:\n            self.log("Attempted to add item without being logged in.", "ERROR")\n            return\n\n        self.connect_db()\n        cursor = self.conn.cursor()\n        cursor.execute("INSERT INTO items (name, value) VALUES (?, ?)", (name, value))\n        self.conn.commit()\n        self.log(f"Item added: {name} with value {value}", "INFO")\n\n    def load_data_into_cache(self):\n        # Load all items from the database into memory\n        self.connect_db()\n        cursor = self.conn.cursor()\n        cursor.execute("SELECT name, value FROM items")\n        self.data_cache = cursor.fetchall()\n        self.log("Data loaded into cache.", "DEBUG")\n\n    def process_data(self):\n        # Process data in memory (e.g., sorting by value)\n        if not self.data_cache:\n            self.log("Data cache is empty, cannot process.", "WARNING")\n            return\n        self.data_cache.sort(key=lambda x: x[1])\n        self.log("Data processed (sorted by value).", "INFO")\n\n    def user_interaction(self):\n        # Simulate user interaction via input/output\n        print("Welcome to the GodClass Application!")\n        username = input("Enter username: ")\n        password = input("Enter password: ")\n        if self.login_user(username, password):\n            print("Login successful.")\n            action = input("Do you want to add an item? (y/n): ")\n            if action.lower() == \'y\':\n                name = input("Item name: ")\n                value = int(input("Item value: "))\n                self.add_item(name, value)\n                print("Item added successfully!")\n            else:\n                print("No action taken.")\n        else:\n            print("Login failed. Goodbye.")\n\n    def close(self):\n        # Close the database connection\n        if self.conn:\n            self.conn.close()\n            self.log("Database connection closed.", "DEBUG")\n            \nclass StringUtils:\n    @staticmethod\n    def trim(s: str) -> str:\n        return s.strip()\n\n    @staticmethod\n    def to_upper(s: str) -> str:\n        return s.upper()\n\n    @staticmethod\n    def to_lower(s: str) -> str:\n        return s.lower()\n\n    @staticmethod\n    def tokenize(s: str, delimiter: str = " ") -> list:\n        return s.split(delimiter)\n\n    @staticmethod\n    def is_alphanumeric(s: str) -> bool:\n        return s.isalnum()\n\n    ', 'Download_Data.py': "\nclass GaTectorDataset(Dataset):\n    def __init__(self, root_dir, mat_file, input_shape, num_classes, train_mode,train):\n        super(GaTectorDataset, self).__init__()\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n        self.train = train\n        self.train_mode=train_mode\n\n        # GOO pickle\n        self.output_size = 64\n        self.input_size = 224\n        self.root_dir = root_dir\n        self.mat_file = mat_file\n        with open(mat_file, 'rb') as f:\n            self.data = pickle.load(f)\n            self.image_num = len(self.data)\n        self.transform = transforms.Compose([transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n                                            ])\n\n    def __len__(self):\n        return self.image_num\n\n    def __getitem__(self, index):\n        index = index % self.image_num\n\n        # GOO pickle\n        data = self.data[index]\n        image_path = data['filename']\n        image_path = os.path.join(self.root_dir, image_path)\n        image_path = image_path.replace('\\', '/')\n        gt_box_idx = data['gazeIdx']\n        # Goo gt_box\n        if self.train_mode==0:\n            gt_bboxes = np.copy(data['ann']['bboxes'])\n            gt_labels = np.copy(data['ann']['labels'])\n        if self.train_mode==1:\n            gt_bboxes = np.copy(data['ann']['bboxes']) / [640, 480, 640, 480] * [1920, 1080, 1920, 1080]\n            gt_labels = np.copy(data['ann']['labels'])\n\n        gt_labels = gt_labels[..., np.newaxis]\n        bbox = np.append(gt_bboxes, gt_labels, axis=1)\n        box = bbox.astype(np.int32)\n\n        gaze_gt_box = box[gt_box_idx]\n        gaze_gt_box = gaze_gt_box[np.newaxis, :]\n\n        # GOO\n        eye = [float(data['hx']) / 640, float(data['hy']) / 480]\n        gaze = [float(data['gaze_cx']) / 640, float(data['gaze_cy']) / 480]\n        img = Image.open(image_path)\n        img = img.convert('RGB')\n        width, height = img.size\n        gaze_x, gaze_y = gaze\n        eye_x, eye_y = eye\n\n        k = 0.1\n        x_min = (eye_x - 0.15) * width\n        y_min = (eye_y - 0.15) * height\n        x_max = (eye_x + 0.15) * width\n        y_max = (eye_y + 0.15) * height\n        if x_min < 0:\n            x_min = 0\n        if y_min < 0:\n            y_min = 0\n        if x_max < 0:\n            x_max = 0\n        if y_max < 0:\n            y_max = 0\n        x_min -= k * abs(x_max - x_min)\n        y_min -= k * abs(y_max - y_min)\n        x_max += k * abs(x_max - x_min)\n        y_max += k * abs(y_max - y_min)\n        x_min, y_min, x_max, y_max = map(float, [x_min, y_min, x_max, y_max])\n\n        if self.train:\n            # data augmentation\n            # Jitter (expansion-only) bounding box size\n            if np.random.random_sample() <= 0.5:\n                k = np.random.random_sample() * 0.2\n                x_min -= k * abs(x_max - x_min)\n                y_min -= k * abs(y_max - y_min)\n                x_max += k * abs(x_max - x_min)\n                y_max += k * abs(y_max - y_min)\n\n            # Random Crop\n            if np.random.random_sample() <= 0.5:\n                # Calculate the minimum valid range of the crop that doesn't exclude the face and the gaze target\n                crop_x_min = np.min([gaze_x * width, x_min, x_max])\n                crop_y_min = np.min([gaze_y * height, y_min, y_max])\n                crop_x_max = np.max([gaze_x * width, x_min, x_max])\n                crop_y_max = np.max([gaze_y * height, y_min, y_max])\n\n                # Randomly select a random top left corner\n                if crop_x_min >= 0:\n                    crop_x_min = np.random.uniform(0, crop_x_min)\n                if crop_y_min >= 0:\n                    crop_y_min = np.random.uniform(0, crop_y_min)\n\n                # Find the range of valid crop width and height starting from the (crop_x_min, crop_y_min)\n                crop_width_min = crop_x_max - crop_x_min\n                crop_height_min = crop_y_max - crop_y_min\n                crop_width_max = width - crop_x_min\n                crop_height_max = height - crop_y_min\n                # Randomly select a width and a height\n                crop_width = np.random.uniform(crop_width_min, crop_width_max)\n                crop_height = np.random.uniform(crop_height_min, crop_height_max)\n\n                # Crop it\n                img = TF.crop(img, crop_y_min, crop_x_min, crop_height, crop_width)\n\n                # Record the crop's (x, y) offset\n                offset_x, offset_y = crop_x_min, crop_y_min\n\n                # convert coordinates into the cropped frame\n                x_min, y_min, x_max, y_max = x_min - offset_x, y_min - offset_y, x_max - offset_x, y_max - offset_y\n                # if gaze_inside:\n                gaze_x, gaze_y = (gaze_x * width - offset_x) / float(crop_width),                                 (gaze_y * height - offset_y) / float(crop_height)\n\n                width, height = crop_width, crop_height\n\n                box[:, [0, 2]] = box[:, [0, 2]] - crop_x_min\n                box[:, [1, 3]] = box[:, [1, 3]] - crop_y_min\n\n                # operate gt_box\n                gaze_gt_box[:, [0, 2]] = gaze_gt_box[:, [0, 2]] - crop_x_min\n                gaze_gt_box[:, [1, 3]] = gaze_gt_box[:, [1, 3]] - crop_y_min\n\n            # Random flip\n            if np.random.random_sample() <= 0.5:\n                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n                x_max_2 = width - x_min\n                x_min_2 = width - x_max\n                x_max = x_max_2\n                x_min = x_min_2\n                gaze_x = 1 - gaze_x\n                box[:, [0, 2]] = width - box[:, [2, 0]]\n\n            # Random color change\n            if np.random.random_sample() <= 0.5:\n                img = TF.adjust_brightness(img, brightness_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_contrast(img, contrast_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_saturation(img, saturation_factor=np.random.uniform(0, 1.5))\n\n            # Random color change\n            if np.random.random_sample() <= 0.5:\n                img = TF.adjust_brightness(img, brightness_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_contrast(img, contrast_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_saturation(img, saturation_factor=np.random.uniform(0, 1.5))\n\n        head_channel = gaze_imutils.get_head_box_channel(x_min, y_min, x_max, y_max, width, height,\n                                                        resolution=self.input_size, coordconv=False).unsqueeze(0)\n\n        # Crop the face\n        face = img.crop((int(x_min), int(y_min), int(x_max), int(y_max)))\n        face = face.resize((self.input_shape), Image.BICUBIC)\n        face = np.transpose(preprocess_input(np.array(face, dtype=np.float32)), (2, 0, 1))\n        face = torch.Tensor(face)\n        face = self.transform(face)\n        img = img.resize((self.input_shape), Image.BICUBIC)\n        img = np.transpose(preprocess_input(np.array(img, dtype=np.float32)), (2, 0, 1))\n        img = torch.Tensor(img)\n        img = self.transform(img)\n\n        # Bbox deal\n        box[:, [0, 2]] = box[:, [0, 2]] * self.input_size / width\n        box[:, [1, 3]] = box[:, [1, 3]] * self.input_size / height\n\n        # operate_gt_box\n        gaze_gt_box[:, [0, 2]] = gaze_gt_box[:, [0, 2]] * self.input_size / width\n        gaze_gt_box[:, [1, 3]] = gaze_gt_box[:, [1, 3]] * self.input_size / height\n\n        box[:, 0:2][box[:, 0:2] < 0] = 0\n        box[:, 2][box[:, 2] > self.input_size] = self.input_size\n        box[:, 3][box[:, 3] > self.input_size] = self.input_size\n        box_w = box[:, 2] - box[:, 0]\n        box_h = box[:, 3] - box[:, 1]\n        box = box[np.logical_and(box_w > 1, box_h > 1)]\n\n        box = np.array(box, dtype=np.float32)\n        if len(box) != 0:\n            box[:, [0, 2]] = box[:, [0, 2]] / self.input_shape[1]\n            box[:, [1, 3]] = box[:, [1, 3]] / self.input_shape[0]\n\n            box[:, 2:4] = box[:, 2:4] - box[:, 0:2]\n            box[:, 0:2] = box[:, 0:2] + box[:, 2:4] / 2\n\n        # generate the heatmap used for deconv prediction\n        gaze_heatmap = torch.zeros(self.output_size, self.output_size)  # set the size of the output\n        gaze_heatmap = gaze_imutils.draw_labelmap(gaze_heatmap, [gaze_x * self.output_size, gaze_y * self.output_size],\n                                                3,\n                                                type='Gaussian')\n        face = np.array(face, dtype=np.float32)\n        img = np.array(img, dtype=np.float32)\n        head_channel = np.array(head_channel, dtype=np.float32)\n        gaze_heatmap = np.array(gaze_heatmap, dtype=np.float32)\n\n        return img, box, face, head_channel, gaze_heatmap, eye, gaze, gaze_gt_box\n\n    def rand(self, a=0, b=1):\n        return np.random.rand() * (b - a) + a\n    "}
2024-12-08 18:16:26,204 - __main__ - INFO - Task job: god_object
2024-12-08 18:16:26,211 - __main__ - ERROR - Error processing task message with correlation ID: 1234
2024-12-08 18:16:26,212 - __main__ - ERROR - 'list' object has no attribute 'items'
2024-12-08 18:21:11,920 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-08 18:21:11,938 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-08 18:21:11,986 - llm_processor - INFO - Models directory: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 18:21:11,986 - llm_processor - INFO - Loading model pipeline: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 18:21:12,428 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-12-08 18:21:14,859 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
2024-12-08 18:21:14,860 - llm_processor - INFO - Model pipeline loaded: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 18:21:14,860 - __main__ - INFO - Initializing knowledge base for detection
2024-12-08 18:21:14,860 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-08 18:21:14,861 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-08 18:21:14,862 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-08 18:21:14,862 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-08 18:21:14,919 - __main__ - INFO - Received task message
2024-12-08 18:21:14,921 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-08 18:21:14,921 - __main__ - INFO - Task type: detection
2024-12-08 18:21:14,922 - __main__ - INFO - Task data: {'Data_Processing.py': '\nclass OrderProcessor:\n    def __init__(self, validator, payment_gateway, notification_service):\n        self.validator = validator\n        self.payment_gateway = payment_gateway\n        self.notification_service = notification_service\n\n    def process_order(self, order):\n        if not self.validator.validate(order):\n            raise ValueError("Order is invalid")\n\n        payment_status = self.payment_gateway.charge(order)\n        if payment_status == "success":\n            self.notification_service.send_confirmation(order)\n        else:\n            self.notification_service.send_failure(order)\n    ', 'Data_Cleaning.py': '\nclass GodClass:\n    def __init__(self, config_path: str, db_path: str):\n        # Configuration\n        self.config = self._load_config(config_path)\n\n        # Database connection\n        self.db_path = db_path\n        self.conn = None\n\n        # Data cache\n        self.data_cache = []\n\n        # Logging settings\n        self.log_file = self.config.get("log_file", "app.log")\n        self.log_level = self.config.get("log_level", "INFO")\n\n        # State\n        self.user_is_logged_in = False\n        self.current_user = None\n\n    def _load_config(self, config_path: str):\n        # Load configuration from a JSON file\n        try:\n            with open(config_path, "r") as f:\n                config = json.load(f)\n        except FileNotFoundError:\n            config = {\n                "log_file": "app.log",\n                "log_level": "INFO",\n                "default_admin": "admin",\n                "default_password": "admin123"\n            }\n        return config\n\n    def log(self, message: str, level: str = "INFO"):\n        # Log a message to a file, ignoring log_level thresholds for simplicity\n        with open(self.log_file, "a") as f:\n            f.write(f"[{level}] {message}\n")\n\n    def connect_db(self):\n        # Connect to the database\n        if self.conn is None:\n            self.conn = sqlite3.connect(self.db_path)\n            self.log("Connected to the database.", "DEBUG")\n\n    def create_tables(self):\n        # Create tables if they don\'t exist\n        self.connect_db()\n        cursor = self.conn.cursor()\n        self.conn.commit()\n        self.log("Tables ensured in database.", "DEBUG")\n\n    def add_user(self, username: str, password: str):\n        # Add a user to the database\n        self.connect_db()\n        try:\n            cursor = self.conn.cursor()\n            cursor.execute("INSERT INTO users (username, password) VALUES (?, ?)", (username, password))\n            self.conn.commit()\n            self.log(f"User added: {username}", "INFO")\n        except sqlite3.IntegrityError:\n            self.log(f"User {username} already exists.", "ERROR")\n\n    def login_user(self, username: str, password: str):\n        # Login a user\n        self.connect_db()\n        cursor = self.conn.cursor()\n        cursor.execute("SELECT * FROM users WHERE username = ? AND password = ?", (username, password))\n        user = cursor.fetchone()\n        if user:\n            self.user_is_logged_in = True\n            self.current_user = username\n            self.log(f"User {username} logged in.", "INFO")\n            return True\n        else:\n            self.log(f"Failed login attempt for user {username}.", "WARNING")\n            return False\n\n    def add_item(self, name: str, value: int):\n        # Add an item to the database\n        if not self.user_is_logged_in:\n            self.log("Attempted to add item without being logged in.", "ERROR")\n            return\n\n        self.connect_db()\n        cursor = self.conn.cursor()\n        cursor.execute("INSERT INTO items (name, value) VALUES (?, ?)", (name, value))\n        self.conn.commit()\n        self.log(f"Item added: {name} with value {value}", "INFO")\n\n    def load_data_into_cache(self):\n        # Load all items from the database into memory\n        self.connect_db()\n        cursor = self.conn.cursor()\n        cursor.execute("SELECT name, value FROM items")\n        self.data_cache = cursor.fetchall()\n        self.log("Data loaded into cache.", "DEBUG")\n\n    def process_data(self):\n        # Process data in memory (e.g., sorting by value)\n        if not self.data_cache:\n            self.log("Data cache is empty, cannot process.", "WARNING")\n            return\n        self.data_cache.sort(key=lambda x: x[1])\n        self.log("Data processed (sorted by value).", "INFO")\n\n    def user_interaction(self):\n        # Simulate user interaction via input/output\n        print("Welcome to the GodClass Application!")\n        username = input("Enter username: ")\n        password = input("Enter password: ")\n        if self.login_user(username, password):\n            print("Login successful.")\n            action = input("Do you want to add an item? (y/n): ")\n            if action.lower() == \'y\':\n                name = input("Item name: ")\n                value = int(input("Item value: "))\n                self.add_item(name, value)\n                print("Item added successfully!")\n            else:\n                print("No action taken.")\n        else:\n            print("Login failed. Goodbye.")\n\n    def close(self):\n        # Close the database connection\n        if self.conn:\n            self.conn.close()\n            self.log("Database connection closed.", "DEBUG")\n            \nclass StringUtils:\n    @staticmethod\n    def trim(s: str) -> str:\n        return s.strip()\n\n    @staticmethod\n    def to_upper(s: str) -> str:\n        return s.upper()\n\n    @staticmethod\n    def to_lower(s: str) -> str:\n        return s.lower()\n\n    @staticmethod\n    def tokenize(s: str, delimiter: str = " ") -> list:\n        return s.split(delimiter)\n\n    @staticmethod\n    def is_alphanumeric(s: str) -> bool:\n        return s.isalnum()\n\n    ', 'Download_Data.py': "\nclass GaTectorDataset(Dataset):\n    def __init__(self, root_dir, mat_file, input_shape, num_classes, train_mode,train):\n        super(GaTectorDataset, self).__init__()\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n        self.train = train\n        self.train_mode=train_mode\n\n        # GOO pickle\n        self.output_size = 64\n        self.input_size = 224\n        self.root_dir = root_dir\n        self.mat_file = mat_file\n        with open(mat_file, 'rb') as f:\n            self.data = pickle.load(f)\n            self.image_num = len(self.data)\n        self.transform = transforms.Compose([transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n                                            ])\n\n    def __len__(self):\n        return self.image_num\n\n    def __getitem__(self, index):\n        index = index % self.image_num\n\n        # GOO pickle\n        data = self.data[index]\n        image_path = data['filename']\n        image_path = os.path.join(self.root_dir, image_path)\n        image_path = image_path.replace('\\', '/')\n        gt_box_idx = data['gazeIdx']\n        # Goo gt_box\n        if self.train_mode==0:\n            gt_bboxes = np.copy(data['ann']['bboxes'])\n            gt_labels = np.copy(data['ann']['labels'])\n        if self.train_mode==1:\n            gt_bboxes = np.copy(data['ann']['bboxes']) / [640, 480, 640, 480] * [1920, 1080, 1920, 1080]\n            gt_labels = np.copy(data['ann']['labels'])\n\n        gt_labels = gt_labels[..., np.newaxis]\n        bbox = np.append(gt_bboxes, gt_labels, axis=1)\n        box = bbox.astype(np.int32)\n\n        gaze_gt_box = box[gt_box_idx]\n        gaze_gt_box = gaze_gt_box[np.newaxis, :]\n\n        # GOO\n        eye = [float(data['hx']) / 640, float(data['hy']) / 480]\n        gaze = [float(data['gaze_cx']) / 640, float(data['gaze_cy']) / 480]\n        img = Image.open(image_path)\n        img = img.convert('RGB')\n        width, height = img.size\n        gaze_x, gaze_y = gaze\n        eye_x, eye_y = eye\n\n        k = 0.1\n        x_min = (eye_x - 0.15) * width\n        y_min = (eye_y - 0.15) * height\n        x_max = (eye_x + 0.15) * width\n        y_max = (eye_y + 0.15) * height\n        if x_min < 0:\n            x_min = 0\n        if y_min < 0:\n            y_min = 0\n        if x_max < 0:\n            x_max = 0\n        if y_max < 0:\n            y_max = 0\n        x_min -= k * abs(x_max - x_min)\n        y_min -= k * abs(y_max - y_min)\n        x_max += k * abs(x_max - x_min)\n        y_max += k * abs(y_max - y_min)\n        x_min, y_min, x_max, y_max = map(float, [x_min, y_min, x_max, y_max])\n\n        if self.train:\n            # data augmentation\n            # Jitter (expansion-only) bounding box size\n            if np.random.random_sample() <= 0.5:\n                k = np.random.random_sample() * 0.2\n                x_min -= k * abs(x_max - x_min)\n                y_min -= k * abs(y_max - y_min)\n                x_max += k * abs(x_max - x_min)\n                y_max += k * abs(y_max - y_min)\n\n            # Random Crop\n            if np.random.random_sample() <= 0.5:\n                # Calculate the minimum valid range of the crop that doesn't exclude the face and the gaze target\n                crop_x_min = np.min([gaze_x * width, x_min, x_max])\n                crop_y_min = np.min([gaze_y * height, y_min, y_max])\n                crop_x_max = np.max([gaze_x * width, x_min, x_max])\n                crop_y_max = np.max([gaze_y * height, y_min, y_max])\n\n                # Randomly select a random top left corner\n                if crop_x_min >= 0:\n                    crop_x_min = np.random.uniform(0, crop_x_min)\n                if crop_y_min >= 0:\n                    crop_y_min = np.random.uniform(0, crop_y_min)\n\n                # Find the range of valid crop width and height starting from the (crop_x_min, crop_y_min)\n                crop_width_min = crop_x_max - crop_x_min\n                crop_height_min = crop_y_max - crop_y_min\n                crop_width_max = width - crop_x_min\n                crop_height_max = height - crop_y_min\n                # Randomly select a width and a height\n                crop_width = np.random.uniform(crop_width_min, crop_width_max)\n                crop_height = np.random.uniform(crop_height_min, crop_height_max)\n\n                # Crop it\n                img = TF.crop(img, crop_y_min, crop_x_min, crop_height, crop_width)\n\n                # Record the crop's (x, y) offset\n                offset_x, offset_y = crop_x_min, crop_y_min\n\n                # convert coordinates into the cropped frame\n                x_min, y_min, x_max, y_max = x_min - offset_x, y_min - offset_y, x_max - offset_x, y_max - offset_y\n                # if gaze_inside:\n                gaze_x, gaze_y = (gaze_x * width - offset_x) / float(crop_width),                                 (gaze_y * height - offset_y) / float(crop_height)\n\n                width, height = crop_width, crop_height\n\n                box[:, [0, 2]] = box[:, [0, 2]] - crop_x_min\n                box[:, [1, 3]] = box[:, [1, 3]] - crop_y_min\n\n                # operate gt_box\n                gaze_gt_box[:, [0, 2]] = gaze_gt_box[:, [0, 2]] - crop_x_min\n                gaze_gt_box[:, [1, 3]] = gaze_gt_box[:, [1, 3]] - crop_y_min\n\n            # Random flip\n            if np.random.random_sample() <= 0.5:\n                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n                x_max_2 = width - x_min\n                x_min_2 = width - x_max\n                x_max = x_max_2\n                x_min = x_min_2\n                gaze_x = 1 - gaze_x\n                box[:, [0, 2]] = width - box[:, [2, 0]]\n\n            # Random color change\n            if np.random.random_sample() <= 0.5:\n                img = TF.adjust_brightness(img, brightness_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_contrast(img, contrast_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_saturation(img, saturation_factor=np.random.uniform(0, 1.5))\n\n            # Random color change\n            if np.random.random_sample() <= 0.5:\n                img = TF.adjust_brightness(img, brightness_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_contrast(img, contrast_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_saturation(img, saturation_factor=np.random.uniform(0, 1.5))\n\n        head_channel = gaze_imutils.get_head_box_channel(x_min, y_min, x_max, y_max, width, height,\n                                                        resolution=self.input_size, coordconv=False).unsqueeze(0)\n\n        # Crop the face\n        face = img.crop((int(x_min), int(y_min), int(x_max), int(y_max)))\n        face = face.resize((self.input_shape), Image.BICUBIC)\n        face = np.transpose(preprocess_input(np.array(face, dtype=np.float32)), (2, 0, 1))\n        face = torch.Tensor(face)\n        face = self.transform(face)\n        img = img.resize((self.input_shape), Image.BICUBIC)\n        img = np.transpose(preprocess_input(np.array(img, dtype=np.float32)), (2, 0, 1))\n        img = torch.Tensor(img)\n        img = self.transform(img)\n\n        # Bbox deal\n        box[:, [0, 2]] = box[:, [0, 2]] * self.input_size / width\n        box[:, [1, 3]] = box[:, [1, 3]] * self.input_size / height\n\n        # operate_gt_box\n        gaze_gt_box[:, [0, 2]] = gaze_gt_box[:, [0, 2]] * self.input_size / width\n        gaze_gt_box[:, [1, 3]] = gaze_gt_box[:, [1, 3]] * self.input_size / height\n\n        box[:, 0:2][box[:, 0:2] < 0] = 0\n        box[:, 2][box[:, 2] > self.input_size] = self.input_size\n        box[:, 3][box[:, 3] > self.input_size] = self.input_size\n        box_w = box[:, 2] - box[:, 0]\n        box_h = box[:, 3] - box[:, 1]\n        box = box[np.logical_and(box_w > 1, box_h > 1)]\n\n        box = np.array(box, dtype=np.float32)\n        if len(box) != 0:\n            box[:, [0, 2]] = box[:, [0, 2]] / self.input_shape[1]\n            box[:, [1, 3]] = box[:, [1, 3]] / self.input_shape[0]\n\n            box[:, 2:4] = box[:, 2:4] - box[:, 0:2]\n            box[:, 0:2] = box[:, 0:2] + box[:, 2:4] / 2\n\n        # generate the heatmap used for deconv prediction\n        gaze_heatmap = torch.zeros(self.output_size, self.output_size)  # set the size of the output\n        gaze_heatmap = gaze_imutils.draw_labelmap(gaze_heatmap, [gaze_x * self.output_size, gaze_y * self.output_size],\n                                                3,\n                                                type='Gaussian')\n        face = np.array(face, dtype=np.float32)\n        img = np.array(img, dtype=np.float32)\n        head_channel = np.array(head_channel, dtype=np.float32)\n        gaze_heatmap = np.array(gaze_heatmap, dtype=np.float32)\n\n        return img, box, face, head_channel, gaze_heatmap, eye, gaze, gaze_gt_box\n\n    def rand(self, a=0, b=1):\n        return np.random.rand() * (b - a) + a\n    "}
2024-12-08 18:21:14,923 - __main__ - INFO - Task job: god_object
2024-12-08 18:22:14,096 - __main__ - ERROR - Error processing task message with correlation ID: 1234
2024-12-08 18:22:14,096 - __main__ - ERROR - 
2024-12-08 18:27:54,036 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-08 18:27:54,061 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-08 18:27:54,115 - llm_processor - INFO - Models directory: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 18:27:54,116 - llm_processor - INFO - Loading model pipeline: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 18:27:54,582 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-12-08 18:27:57,034 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
2024-12-08 18:27:57,035 - llm_processor - INFO - Model pipeline loaded: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 18:27:57,035 - __main__ - INFO - Initializing knowledge base for detection
2024-12-08 18:27:57,036 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-08 18:27:57,036 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-08 18:27:57,037 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-08 18:27:57,037 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-08 18:28:27,859 - __main__ - INFO - Received task message
2024-12-08 18:28:27,859 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-08 18:28:27,860 - __main__ - INFO - Task type: detection
2024-12-08 18:28:27,860 - __main__ - INFO - Task data: {'Data_Processing.py': '\nclass OrderProcessor:\n    def __init__(self, validator, payment_gateway, notification_service):\n        self.validator = validator\n        self.payment_gateway = payment_gateway\n        self.notification_service = notification_service\n\n    def process_order(self, order):\n        if not self.validator.validate(order):\n            raise ValueError("Order is invalid")\n\n        payment_status = self.payment_gateway.charge(order)\n        if payment_status == "success":\n            self.notification_service.send_confirmation(order)\n        else:\n            self.notification_service.send_failure(order)\n    ', 'Data_Cleaning.py': '\nclass GodClass:\n    def __init__(self, config_path: str, db_path: str):\n        # Configuration\n        self.config = self._load_config(config_path)\n\n        # Database connection\n        self.db_path = db_path\n        self.conn = None\n\n        # Data cache\n        self.data_cache = []\n\n        # Logging settings\n        self.log_file = self.config.get("log_file", "app.log")\n        self.log_level = self.config.get("log_level", "INFO")\n\n        # State\n        self.user_is_logged_in = False\n        self.current_user = None\n\n    def _load_config(self, config_path: str):\n        # Load configuration from a JSON file\n        try:\n            with open(config_path, "r") as f:\n                config = json.load(f)\n        except FileNotFoundError:\n            config = {\n                "log_file": "app.log",\n                "log_level": "INFO",\n                "default_admin": "admin",\n                "default_password": "admin123"\n            }\n        return config\n\n    def log(self, message: str, level: str = "INFO"):\n        # Log a message to a file, ignoring log_level thresholds for simplicity\n        with open(self.log_file, "a") as f:\n            f.write(f"[{level}] {message}\n")\n\n    def connect_db(self):\n        # Connect to the database\n        if self.conn is None:\n            self.conn = sqlite3.connect(self.db_path)\n            self.log("Connected to the database.", "DEBUG")\n\n    def create_tables(self):\n        # Create tables if they don\'t exist\n        self.connect_db()\n        cursor = self.conn.cursor()\n        self.conn.commit()\n        self.log("Tables ensured in database.", "DEBUG")\n\n    def add_user(self, username: str, password: str):\n        # Add a user to the database\n        self.connect_db()\n        try:\n            cursor = self.conn.cursor()\n            cursor.execute("INSERT INTO users (username, password) VALUES (?, ?)", (username, password))\n            self.conn.commit()\n            self.log(f"User added: {username}", "INFO")\n        except sqlite3.IntegrityError:\n            self.log(f"User {username} already exists.", "ERROR")\n\n    def login_user(self, username: str, password: str):\n        # Login a user\n        self.connect_db()\n        cursor = self.conn.cursor()\n        cursor.execute("SELECT * FROM users WHERE username = ? AND password = ?", (username, password))\n        user = cursor.fetchone()\n        if user:\n            self.user_is_logged_in = True\n            self.current_user = username\n            self.log(f"User {username} logged in.", "INFO")\n            return True\n        else:\n            self.log(f"Failed login attempt for user {username}.", "WARNING")\n            return False\n\n    def add_item(self, name: str, value: int):\n        # Add an item to the database\n        if not self.user_is_logged_in:\n            self.log("Attempted to add item without being logged in.", "ERROR")\n            return\n\n        self.connect_db()\n        cursor = self.conn.cursor()\n        cursor.execute("INSERT INTO items (name, value) VALUES (?, ?)", (name, value))\n        self.conn.commit()\n        self.log(f"Item added: {name} with value {value}", "INFO")\n\n    def load_data_into_cache(self):\n        # Load all items from the database into memory\n        self.connect_db()\n        cursor = self.conn.cursor()\n        cursor.execute("SELECT name, value FROM items")\n        self.data_cache = cursor.fetchall()\n        self.log("Data loaded into cache.", "DEBUG")\n\n    def process_data(self):\n        # Process data in memory (e.g., sorting by value)\n        if not self.data_cache:\n            self.log("Data cache is empty, cannot process.", "WARNING")\n            return\n        self.data_cache.sort(key=lambda x: x[1])\n        self.log("Data processed (sorted by value).", "INFO")\n\n    def user_interaction(self):\n        # Simulate user interaction via input/output\n        print("Welcome to the GodClass Application!")\n        username = input("Enter username: ")\n        password = input("Enter password: ")\n        if self.login_user(username, password):\n            print("Login successful.")\n            action = input("Do you want to add an item? (y/n): ")\n            if action.lower() == \'y\':\n                name = input("Item name: ")\n                value = int(input("Item value: "))\n                self.add_item(name, value)\n                print("Item added successfully!")\n            else:\n                print("No action taken.")\n        else:\n            print("Login failed. Goodbye.")\n\n    def close(self):\n        # Close the database connection\n        if self.conn:\n            self.conn.close()\n            self.log("Database connection closed.", "DEBUG")\n            \nclass StringUtils:\n    @staticmethod\n    def trim(s: str) -> str:\n        return s.strip()\n\n    @staticmethod\n    def to_upper(s: str) -> str:\n        return s.upper()\n\n    @staticmethod\n    def to_lower(s: str) -> str:\n        return s.lower()\n\n    @staticmethod\n    def tokenize(s: str, delimiter: str = " ") -> list:\n        return s.split(delimiter)\n\n    @staticmethod\n    def is_alphanumeric(s: str) -> bool:\n        return s.isalnum()\n\n    ', 'Download_Data.py': "\nclass GaTectorDataset(Dataset):\n    def __init__(self, root_dir, mat_file, input_shape, num_classes, train_mode,train):\n        super(GaTectorDataset, self).__init__()\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n        self.train = train\n        self.train_mode=train_mode\n\n        # GOO pickle\n        self.output_size = 64\n        self.input_size = 224\n        self.root_dir = root_dir\n        self.mat_file = mat_file\n        with open(mat_file, 'rb') as f:\n            self.data = pickle.load(f)\n            self.image_num = len(self.data)\n        self.transform = transforms.Compose([transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n                                            ])\n\n    def __len__(self):\n        return self.image_num\n\n    def __getitem__(self, index):\n        index = index % self.image_num\n\n        # GOO pickle\n        data = self.data[index]\n        image_path = data['filename']\n        image_path = os.path.join(self.root_dir, image_path)\n        image_path = image_path.replace('\\', '/')\n        gt_box_idx = data['gazeIdx']\n        # Goo gt_box\n        if self.train_mode==0:\n            gt_bboxes = np.copy(data['ann']['bboxes'])\n            gt_labels = np.copy(data['ann']['labels'])\n        if self.train_mode==1:\n            gt_bboxes = np.copy(data['ann']['bboxes']) / [640, 480, 640, 480] * [1920, 1080, 1920, 1080]\n            gt_labels = np.copy(data['ann']['labels'])\n\n        gt_labels = gt_labels[..., np.newaxis]\n        bbox = np.append(gt_bboxes, gt_labels, axis=1)\n        box = bbox.astype(np.int32)\n\n        gaze_gt_box = box[gt_box_idx]\n        gaze_gt_box = gaze_gt_box[np.newaxis, :]\n\n        # GOO\n        eye = [float(data['hx']) / 640, float(data['hy']) / 480]\n        gaze = [float(data['gaze_cx']) / 640, float(data['gaze_cy']) / 480]\n        img = Image.open(image_path)\n        img = img.convert('RGB')\n        width, height = img.size\n        gaze_x, gaze_y = gaze\n        eye_x, eye_y = eye\n\n        k = 0.1\n        x_min = (eye_x - 0.15) * width\n        y_min = (eye_y - 0.15) * height\n        x_max = (eye_x + 0.15) * width\n        y_max = (eye_y + 0.15) * height\n        if x_min < 0:\n            x_min = 0\n        if y_min < 0:\n            y_min = 0\n        if x_max < 0:\n            x_max = 0\n        if y_max < 0:\n            y_max = 0\n        x_min -= k * abs(x_max - x_min)\n        y_min -= k * abs(y_max - y_min)\n        x_max += k * abs(x_max - x_min)\n        y_max += k * abs(y_max - y_min)\n        x_min, y_min, x_max, y_max = map(float, [x_min, y_min, x_max, y_max])\n\n        if self.train:\n            # data augmentation\n            # Jitter (expansion-only) bounding box size\n            if np.random.random_sample() <= 0.5:\n                k = np.random.random_sample() * 0.2\n                x_min -= k * abs(x_max - x_min)\n                y_min -= k * abs(y_max - y_min)\n                x_max += k * abs(x_max - x_min)\n                y_max += k * abs(y_max - y_min)\n\n            # Random Crop\n            if np.random.random_sample() <= 0.5:\n                # Calculate the minimum valid range of the crop that doesn't exclude the face and the gaze target\n                crop_x_min = np.min([gaze_x * width, x_min, x_max])\n                crop_y_min = np.min([gaze_y * height, y_min, y_max])\n                crop_x_max = np.max([gaze_x * width, x_min, x_max])\n                crop_y_max = np.max([gaze_y * height, y_min, y_max])\n\n                # Randomly select a random top left corner\n                if crop_x_min >= 0:\n                    crop_x_min = np.random.uniform(0, crop_x_min)\n                if crop_y_min >= 0:\n                    crop_y_min = np.random.uniform(0, crop_y_min)\n\n                # Find the range of valid crop width and height starting from the (crop_x_min, crop_y_min)\n                crop_width_min = crop_x_max - crop_x_min\n                crop_height_min = crop_y_max - crop_y_min\n                crop_width_max = width - crop_x_min\n                crop_height_max = height - crop_y_min\n                # Randomly select a width and a height\n                crop_width = np.random.uniform(crop_width_min, crop_width_max)\n                crop_height = np.random.uniform(crop_height_min, crop_height_max)\n\n                # Crop it\n                img = TF.crop(img, crop_y_min, crop_x_min, crop_height, crop_width)\n\n                # Record the crop's (x, y) offset\n                offset_x, offset_y = crop_x_min, crop_y_min\n\n                # convert coordinates into the cropped frame\n                x_min, y_min, x_max, y_max = x_min - offset_x, y_min - offset_y, x_max - offset_x, y_max - offset_y\n                # if gaze_inside:\n                gaze_x, gaze_y = (gaze_x * width - offset_x) / float(crop_width),                                 (gaze_y * height - offset_y) / float(crop_height)\n\n                width, height = crop_width, crop_height\n\n                box[:, [0, 2]] = box[:, [0, 2]] - crop_x_min\n                box[:, [1, 3]] = box[:, [1, 3]] - crop_y_min\n\n                # operate gt_box\n                gaze_gt_box[:, [0, 2]] = gaze_gt_box[:, [0, 2]] - crop_x_min\n                gaze_gt_box[:, [1, 3]] = gaze_gt_box[:, [1, 3]] - crop_y_min\n\n            # Random flip\n            if np.random.random_sample() <= 0.5:\n                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n                x_max_2 = width - x_min\n                x_min_2 = width - x_max\n                x_max = x_max_2\n                x_min = x_min_2\n                gaze_x = 1 - gaze_x\n                box[:, [0, 2]] = width - box[:, [2, 0]]\n\n            # Random color change\n            if np.random.random_sample() <= 0.5:\n                img = TF.adjust_brightness(img, brightness_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_contrast(img, contrast_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_saturation(img, saturation_factor=np.random.uniform(0, 1.5))\n\n            # Random color change\n            if np.random.random_sample() <= 0.5:\n                img = TF.adjust_brightness(img, brightness_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_contrast(img, contrast_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_saturation(img, saturation_factor=np.random.uniform(0, 1.5))\n\n        head_channel = gaze_imutils.get_head_box_channel(x_min, y_min, x_max, y_max, width, height,\n                                                        resolution=self.input_size, coordconv=False).unsqueeze(0)\n\n        # Crop the face\n        face = img.crop((int(x_min), int(y_min), int(x_max), int(y_max)))\n        face = face.resize((self.input_shape), Image.BICUBIC)\n        face = np.transpose(preprocess_input(np.array(face, dtype=np.float32)), (2, 0, 1))\n        face = torch.Tensor(face)\n        face = self.transform(face)\n        img = img.resize((self.input_shape), Image.BICUBIC)\n        img = np.transpose(preprocess_input(np.array(img, dtype=np.float32)), (2, 0, 1))\n        img = torch.Tensor(img)\n        img = self.transform(img)\n\n        # Bbox deal\n        box[:, [0, 2]] = box[:, [0, 2]] * self.input_size / width\n        box[:, [1, 3]] = box[:, [1, 3]] * self.input_size / height\n\n        # operate_gt_box\n        gaze_gt_box[:, [0, 2]] = gaze_gt_box[:, [0, 2]] * self.input_size / width\n        gaze_gt_box[:, [1, 3]] = gaze_gt_box[:, [1, 3]] * self.input_size / height\n\n        box[:, 0:2][box[:, 0:2] < 0] = 0\n        box[:, 2][box[:, 2] > self.input_size] = self.input_size\n        box[:, 3][box[:, 3] > self.input_size] = self.input_size\n        box_w = box[:, 2] - box[:, 0]\n        box_h = box[:, 3] - box[:, 1]\n        box = box[np.logical_and(box_w > 1, box_h > 1)]\n\n        box = np.array(box, dtype=np.float32)\n        if len(box) != 0:\n            box[:, [0, 2]] = box[:, [0, 2]] / self.input_shape[1]\n            box[:, [1, 3]] = box[:, [1, 3]] / self.input_shape[0]\n\n            box[:, 2:4] = box[:, 2:4] - box[:, 0:2]\n            box[:, 0:2] = box[:, 0:2] + box[:, 2:4] / 2\n\n        # generate the heatmap used for deconv prediction\n        gaze_heatmap = torch.zeros(self.output_size, self.output_size)  # set the size of the output\n        gaze_heatmap = gaze_imutils.draw_labelmap(gaze_heatmap, [gaze_x * self.output_size, gaze_y * self.output_size],\n                                                3,\n                                                type='Gaussian')\n        face = np.array(face, dtype=np.float32)\n        img = np.array(img, dtype=np.float32)\n        head_channel = np.array(head_channel, dtype=np.float32)\n        gaze_heatmap = np.array(gaze_heatmap, dtype=np.float32)\n\n        return img, box, face, head_channel, gaze_heatmap, eye, gaze, gaze_gt_box\n\n    def rand(self, a=0, b=1):\n        return np.random.rand() * (b - a) + a\n    "}
2024-12-08 18:28:27,861 - __main__ - INFO - Task job: god_object
2024-12-08 18:33:51,485 - __main__ - ERROR - Error processing task message with correlation ID: 1234
2024-12-08 18:33:51,486 - __main__ - ERROR - 
2024-12-08 18:34:11,088 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-08 18:34:11,112 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-08 18:34:11,165 - llm_processor - INFO - Models directory: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 18:34:11,165 - llm_processor - INFO - Loading model pipeline: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 18:34:11,614 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-12-08 18:34:13,919 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
2024-12-08 18:34:13,920 - llm_processor - INFO - Model pipeline loaded: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 18:34:13,921 - __main__ - INFO - Initializing knowledge base for detection
2024-12-08 18:34:13,921 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-08 18:34:13,921 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-08 18:34:13,922 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-08 18:34:13,922 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-08 18:34:17,170 - __main__ - INFO - Received task message
2024-12-08 18:34:17,171 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-08 18:34:17,171 - __main__ - INFO - Task type: detection
2024-12-08 18:34:17,172 - __main__ - INFO - Task data: {'Data_Processing.py': '\nclass OrderProcessor:\n    def __init__(self, validator, payment_gateway, notification_service):\n        self.validator = validator\n        self.payment_gateway = payment_gateway\n        self.notification_service = notification_service\n\n    def process_order(self, order):\n        if not self.validator.validate(order):\n            raise ValueError("Order is invalid")\n\n        payment_status = self.payment_gateway.charge(order)\n        if payment_status == "success":\n            self.notification_service.send_confirmation(order)\n        else:\n            self.notification_service.send_failure(order)\n    ', 'Data_Cleaning.py': '\nclass GodClass:\n    def __init__(self, config_path: str, db_path: str):\n        # Configuration\n        self.config = self._load_config(config_path)\n\n        # Database connection\n        self.db_path = db_path\n        self.conn = None\n\n        # Data cache\n        self.data_cache = []\n\n        # Logging settings\n        self.log_file = self.config.get("log_file", "app.log")\n        self.log_level = self.config.get("log_level", "INFO")\n\n        # State\n        self.user_is_logged_in = False\n        self.current_user = None\n\n    def _load_config(self, config_path: str):\n        # Load configuration from a JSON file\n        try:\n            with open(config_path, "r") as f:\n                config = json.load(f)\n        except FileNotFoundError:\n            config = {\n                "log_file": "app.log",\n                "log_level": "INFO",\n                "default_admin": "admin",\n                "default_password": "admin123"\n            }\n        return config\n\n    def log(self, message: str, level: str = "INFO"):\n        # Log a message to a file, ignoring log_level thresholds for simplicity\n        with open(self.log_file, "a") as f:\n            f.write(f"[{level}] {message}\n")\n\n    def connect_db(self):\n        # Connect to the database\n        if self.conn is None:\n            self.conn = sqlite3.connect(self.db_path)\n            self.log("Connected to the database.", "DEBUG")\n\n    def create_tables(self):\n        # Create tables if they don\'t exist\n        self.connect_db()\n        cursor = self.conn.cursor()\n        self.conn.commit()\n        self.log("Tables ensured in database.", "DEBUG")\n\n    def add_user(self, username: str, password: str):\n        # Add a user to the database\n        self.connect_db()\n        try:\n            cursor = self.conn.cursor()\n            cursor.execute("INSERT INTO users (username, password) VALUES (?, ?)", (username, password))\n            self.conn.commit()\n            self.log(f"User added: {username}", "INFO")\n        except sqlite3.IntegrityError:\n            self.log(f"User {username} already exists.", "ERROR")\n\n    def login_user(self, username: str, password: str):\n        # Login a user\n        self.connect_db()\n        cursor = self.conn.cursor()\n        cursor.execute("SELECT * FROM users WHERE username = ? AND password = ?", (username, password))\n        user = cursor.fetchone()\n        if user:\n            self.user_is_logged_in = True\n            self.current_user = username\n            self.log(f"User {username} logged in.", "INFO")\n            return True\n        else:\n            self.log(f"Failed login attempt for user {username}.", "WARNING")\n            return False\n\n    def add_item(self, name: str, value: int):\n        # Add an item to the database\n        if not self.user_is_logged_in:\n            self.log("Attempted to add item without being logged in.", "ERROR")\n            return\n\n        self.connect_db()\n        cursor = self.conn.cursor()\n        cursor.execute("INSERT INTO items (name, value) VALUES (?, ?)", (name, value))\n        self.conn.commit()\n        self.log(f"Item added: {name} with value {value}", "INFO")\n\n    def load_data_into_cache(self):\n        # Load all items from the database into memory\n        self.connect_db()\n        cursor = self.conn.cursor()\n        cursor.execute("SELECT name, value FROM items")\n        self.data_cache = cursor.fetchall()\n        self.log("Data loaded into cache.", "DEBUG")\n\n    def process_data(self):\n        # Process data in memory (e.g., sorting by value)\n        if not self.data_cache:\n            self.log("Data cache is empty, cannot process.", "WARNING")\n            return\n        self.data_cache.sort(key=lambda x: x[1])\n        self.log("Data processed (sorted by value).", "INFO")\n\n    def user_interaction(self):\n        # Simulate user interaction via input/output\n        print("Welcome to the GodClass Application!")\n        username = input("Enter username: ")\n        password = input("Enter password: ")\n        if self.login_user(username, password):\n            print("Login successful.")\n            action = input("Do you want to add an item? (y/n): ")\n            if action.lower() == \'y\':\n                name = input("Item name: ")\n                value = int(input("Item value: "))\n                self.add_item(name, value)\n                print("Item added successfully!")\n            else:\n                print("No action taken.")\n        else:\n            print("Login failed. Goodbye.")\n\n    def close(self):\n        # Close the database connection\n        if self.conn:\n            self.conn.close()\n            self.log("Database connection closed.", "DEBUG")\n            \nclass StringUtils:\n    @staticmethod\n    def trim(s: str) -> str:\n        return s.strip()\n\n    @staticmethod\n    def to_upper(s: str) -> str:\n        return s.upper()\n\n    @staticmethod\n    def to_lower(s: str) -> str:\n        return s.lower()\n\n    @staticmethod\n    def tokenize(s: str, delimiter: str = " ") -> list:\n        return s.split(delimiter)\n\n    @staticmethod\n    def is_alphanumeric(s: str) -> bool:\n        return s.isalnum()\n\n    ', 'Download_Data.py': "\nclass GaTectorDataset(Dataset):\n    def __init__(self, root_dir, mat_file, input_shape, num_classes, train_mode,train):\n        super(GaTectorDataset, self).__init__()\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n        self.train = train\n        self.train_mode=train_mode\n\n        # GOO pickle\n        self.output_size = 64\n        self.input_size = 224\n        self.root_dir = root_dir\n        self.mat_file = mat_file\n        with open(mat_file, 'rb') as f:\n            self.data = pickle.load(f)\n            self.image_num = len(self.data)\n        self.transform = transforms.Compose([transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n                                            ])\n\n    def __len__(self):\n        return self.image_num\n\n    def __getitem__(self, index):\n        index = index % self.image_num\n\n        # GOO pickle\n        data = self.data[index]\n        image_path = data['filename']\n        image_path = os.path.join(self.root_dir, image_path)\n        image_path = image_path.replace('\\\\', '/')\n        gt_box_idx = data['gazeIdx']\n        # Goo gt_box\n        if self.train_mode==0:\n            gt_bboxes = np.copy(data['ann']['bboxes'])\n            gt_labels = np.copy(data['ann']['labels'])\n        if self.train_mode==1:\n            gt_bboxes = np.copy(data['ann']['bboxes']) / [640, 480, 640, 480] * [1920, 1080, 1920, 1080]\n            gt_labels = np.copy(data['ann']['labels'])\n\n        gt_labels = gt_labels[..., np.newaxis]\n        bbox = np.append(gt_bboxes, gt_labels, axis=1)\n        box = bbox.astype(np.int32)\n\n        gaze_gt_box = box[gt_box_idx]\n        gaze_gt_box = gaze_gt_box[np.newaxis, :]\n\n        # GOO\n        eye = [float(data['hx']) / 640, float(data['hy']) / 480]\n        gaze = [float(data['gaze_cx']) / 640, float(data['gaze_cy']) / 480]\n        img = Image.open(image_path)\n        img = img.convert('RGB')\n        width, height = img.size\n        gaze_x, gaze_y = gaze\n        eye_x, eye_y = eye\n\n        k = 0.1\n        x_min = (eye_x - 0.15) * width\n        y_min = (eye_y - 0.15) * height\n        x_max = (eye_x + 0.15) * width\n        y_max = (eye_y + 0.15) * height\n        if x_min < 0:\n            x_min = 0\n        if y_min < 0:\n            y_min = 0\n        if x_max < 0:\n            x_max = 0\n        if y_max < 0:\n            y_max = 0\n        x_min -= k * abs(x_max - x_min)\n        y_min -= k * abs(y_max - y_min)\n        x_max += k * abs(x_max - x_min)\n        y_max += k * abs(y_max - y_min)\n        x_min, y_min, x_max, y_max = map(float, [x_min, y_min, x_max, y_max])\n\n        if self.train:\n            # data augmentation\n            # Jitter (expansion-only) bounding box size\n            if np.random.random_sample() <= 0.5:\n                k = np.random.random_sample() * 0.2\n                x_min -= k * abs(x_max - x_min)\n                y_min -= k * abs(y_max - y_min)\n                x_max += k * abs(x_max - x_min)\n                y_max += k * abs(y_max - y_min)\n\n            # Random Crop\n            if np.random.random_sample() <= 0.5:\n                # Calculate the minimum valid range of the crop that doesn't exclude the face and the gaze target\n                crop_x_min = np.min([gaze_x * width, x_min, x_max])\n                crop_y_min = np.min([gaze_y * height, y_min, y_max])\n                crop_x_max = np.max([gaze_x * width, x_min, x_max])\n                crop_y_max = np.max([gaze_y * height, y_min, y_max])\n\n                # Randomly select a random top left corner\n                if crop_x_min >= 0:\n                    crop_x_min = np.random.uniform(0, crop_x_min)\n                if crop_y_min >= 0:\n                    crop_y_min = np.random.uniform(0, crop_y_min)\n\n                # Find the range of valid crop width and height starting from the (crop_x_min, crop_y_min)\n                crop_width_min = crop_x_max - crop_x_min\n                crop_height_min = crop_y_max - crop_y_min\n                crop_width_max = width - crop_x_min\n                crop_height_max = height - crop_y_min\n                # Randomly select a width and a height\n                crop_width = np.random.uniform(crop_width_min, crop_width_max)\n                crop_height = np.random.uniform(crop_height_min, crop_height_max)\n\n                # Crop it\n                img = TF.crop(img, crop_y_min, crop_x_min, crop_height, crop_width)\n\n                # Record the crop's (x, y) offset\n                offset_x, offset_y = crop_x_min, crop_y_min\n\n                # convert coordinates into the cropped frame\n                x_min, y_min, x_max, y_max = x_min - offset_x, y_min - offset_y, x_max - offset_x, y_max - offset_y\n                # if gaze_inside:\n                gaze_x, gaze_y = (gaze_x * width - offset_x) / float(crop_width),                                 (gaze_y * height - offset_y) / float(crop_height)\n\n                width, height = crop_width, crop_height\n\n                box[:, [0, 2]] = box[:, [0, 2]] - crop_x_min\n                box[:, [1, 3]] = box[:, [1, 3]] - crop_y_min\n\n                # operate gt_box\n                gaze_gt_box[:, [0, 2]] = gaze_gt_box[:, [0, 2]] - crop_x_min\n                gaze_gt_box[:, [1, 3]] = gaze_gt_box[:, [1, 3]] - crop_y_min\n\n            # Random flip\n            if np.random.random_sample() <= 0.5:\n                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n                x_max_2 = width - x_min\n                x_min_2 = width - x_max\n                x_max = x_max_2\n                x_min = x_min_2\n                gaze_x = 1 - gaze_x\n                box[:, [0, 2]] = width - box[:, [2, 0]]\n\n            # Random color change\n            if np.random.random_sample() <= 0.5:\n                img = TF.adjust_brightness(img, brightness_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_contrast(img, contrast_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_saturation(img, saturation_factor=np.random.uniform(0, 1.5))\n\n            # Random color change\n            if np.random.random_sample() <= 0.5:\n                img = TF.adjust_brightness(img, brightness_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_contrast(img, contrast_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_saturation(img, saturation_factor=np.random.uniform(0, 1.5))\n\n        head_channel = gaze_imutils.get_head_box_channel(x_min, y_min, x_max, y_max, width, height,\n                                                        resolution=self.input_size, coordconv=False).unsqueeze(0)\n\n        # Crop the face\n        face = img.crop((int(x_min), int(y_min), int(x_max), int(y_max)))\n        face = face.resize((self.input_shape), Image.BICUBIC)\n        face = np.transpose(preprocess_input(np.array(face, dtype=np.float32)), (2, 0, 1))\n        face = torch.Tensor(face)\n        face = self.transform(face)\n        img = img.resize((self.input_shape), Image.BICUBIC)\n        img = np.transpose(preprocess_input(np.array(img, dtype=np.float32)), (2, 0, 1))\n        img = torch.Tensor(img)\n        img = self.transform(img)\n\n        # Bbox deal\n        box[:, [0, 2]] = box[:, [0, 2]] * self.input_size / width\n        box[:, [1, 3]] = box[:, [1, 3]] * self.input_size / height\n\n        # operate_gt_box\n        gaze_gt_box[:, [0, 2]] = gaze_gt_box[:, [0, 2]] * self.input_size / width\n        gaze_gt_box[:, [1, 3]] = gaze_gt_box[:, [1, 3]] * self.input_size / height\n\n        box[:, 0:2][box[:, 0:2] < 0] = 0\n        box[:, 2][box[:, 2] > self.input_size] = self.input_size\n        box[:, 3][box[:, 3] > self.input_size] = self.input_size\n        box_w = box[:, 2] - box[:, 0]\n        box_h = box[:, 3] - box[:, 1]\n        box = box[np.logical_and(box_w > 1, box_h > 1)]\n\n        box = np.array(box, dtype=np.float32)\n        if len(box) != 0:\n            box[:, [0, 2]] = box[:, [0, 2]] / self.input_shape[1]\n            box[:, [1, 3]] = box[:, [1, 3]] / self.input_shape[0]\n\n            box[:, 2:4] = box[:, 2:4] - box[:, 0:2]\n            box[:, 0:2] = box[:, 0:2] + box[:, 2:4] / 2\n\n        # generate the heatmap used for deconv prediction\n        gaze_heatmap = torch.zeros(self.output_size, self.output_size)  # set the size of the output\n        gaze_heatmap = gaze_imutils.draw_labelmap(gaze_heatmap, [gaze_x * self.output_size, gaze_y * self.output_size],\n                                                3,\n                                                type='Gaussian')\n        face = np.array(face, dtype=np.float32)\n        img = np.array(img, dtype=np.float32)\n        head_channel = np.array(head_channel, dtype=np.float32)\n        gaze_heatmap = np.array(gaze_heatmap, dtype=np.float32)\n\n        return img, box, face, head_channel, gaze_heatmap, eye, gaze, gaze_gt_box\n\n    def rand(self, a=0, b=1):\n        return np.random.rand() * (b - a) + a\n    "}
2024-12-08 18:34:17,173 - __main__ - INFO - Task job: god_object
2024-12-08 18:35:59,564 - __main__ - ERROR - Error processing task message with correlation ID: 1234
2024-12-08 18:35:59,565 - __main__ - ERROR - 
2024-12-08 18:39:59,707 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-08 18:39:59,737 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-08 18:39:59,793 - llm_processor - INFO - Models directory: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 18:39:59,794 - llm_processor - INFO - Loading model pipeline: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 18:40:00,258 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-12-08 18:40:02,822 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
2024-12-08 18:40:02,824 - llm_processor - INFO - Model pipeline loaded: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 18:40:02,824 - __main__ - INFO - Initializing knowledge base for detection
2024-12-08 18:40:02,824 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-08 18:40:02,824 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-08 18:40:02,825 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-08 18:40:02,825 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-08 18:41:29,869 - __main__ - INFO - Received task message
2024-12-08 18:41:29,870 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-08 18:41:29,870 - __main__ - INFO - Task type: detection
2024-12-08 18:41:29,871 - __main__ - INFO - Task data: {'Data_Processing.py': 'class OrderProcessor:\n    def __init__(self, validator, payment_gateway, notification_service):\n        self.validator = validator\n        self.payment_gateway = payment_gateway\n        self.notification_service = notification_service\n\n    def process_order(self, order):\n        if not self.validator.validate(order):\n            raise ValueError("Order is invalid")\n\n        payment_status = self.payment_gateway.charge(order)\n        if payment_status == "success":\n            self.notification_service.send_confirmation(order)\n        else:\n            self.notification_service.send_failure(order)\n', 'Data_Cleaning.py': 'class GodClass:\n    def __init__(self, config_path: str, db_path: str):\n        # Configuration\n        self.config = self._load_config(config_path)\n\n        # Database connection\n        self.db_path = db_path\n        self.conn = None\n\n        # Data cache\n        self.data_cache = []\n\n        # Logging settings\n        self.log_file = self.config.get("log_file", "app.log")\n        self.log_level = self.config.get("log_level", "INFO")\n\n        # State\n        self.user_is_logged_in = False\n        self.current_user = None\n\n    def _load_config(self, config_path: str):\n        # Load configuration from a JSON file\n        try:\n            with open(config_path, "r") as f:\n                config = json.load(f)\n        except FileNotFoundError:\n            config = {\n                "log_file": "app.log",\n                "log_level": "INFO",\n                "default_admin": "admin",\n                "default_password": "admin123"\n            }\n        return config\n\n    def log(self, message: str, level: str = "INFO"):\n        # Log a message to a file, ignoring log_level thresholds for simplicity\n        with open(self.log_file, "a") as f:\n            f.write(f"[{level}] {message}\\n")\n\n    def connect_db(self):\n        # Connect to the database\n        if self.conn is None:\n            self.conn = sqlite3.connect(self.db_path)\n            self.log("Connected to the database.", "DEBUG")\n\n    def create_tables(self):\n        # Create tables if they don\'t exist\n        self.connect_db()\n        cursor = self.conn.cursor()\n        self.conn.commit()\n        self.log("Tables ensured in database.", "DEBUG")\n\n    def add_user(self, username: str, password: str):\n        # Add a user to the database\n        self.connect_db()\n        try:\n            cursor = self.conn.cursor()\n            cursor.execute("INSERT INTO users (username, password) VALUES (?, ?)", (username, password))\n            self.conn.commit()\n            self.log(f"User added: {username}", "INFO")\n        except sqlite3.IntegrityError:\n            self.log(f"User {username} already exists.", "ERROR")\n\n    def login_user(self, username: str, password: str):\n        # Login a user\n        self.connect_db()\n        cursor = self.conn.cursor()\n        cursor.execute("SELECT * FROM users WHERE username = ? AND password = ?", (username, password))\n        user = cursor.fetchone()\n        if user:\n            self.user_is_logged_in = True\n            self.current_user = username\n            self.log(f"User {username} logged in.", "INFO")\n            return True\n        else:\n            self.log(f"Failed login attempt for user {username}.", "WARNING")\n            return False\n\n    def add_item(self, name: str, value: int):\n        # Add an item to the database\n        if not self.user_is_logged_in:\n            self.log("Attempted to add item without being logged in.", "ERROR")\n            return\n\n        self.connect_db()\n        cursor = self.conn.cursor()\n        cursor.execute("INSERT INTO items (name, value) VALUES (?, ?)", (name, value))\n        self.conn.commit()\n        self.log(f"Item added: {name} with value {value}", "INFO")\n\n    def load_data_into_cache(self):\n        # Load all items from the database into memory\n        self.connect_db()\n        cursor = self.conn.cursor()\n        cursor.execute("SELECT name, value FROM items")\n        self.data_cache = cursor.fetchall()\n        self.log("Data loaded into cache.", "DEBUG")\n\n    def process_data(self):\n        # Process data in memory (e.g., sorting by value)\n        if not self.data_cache:\n            self.log("Data cache is empty, cannot process.", "WARNING")\n            return\n        self.data_cache.sort(key=lambda x: x[1])\n        self.log("Data processed (sorted by value).", "INFO")\n\n    def user_interaction(self):\n        # Simulate user interaction via input/output\n        print("Welcome to the GodClass Application!")\n        username = input("Enter username: ")\n        password = input("Enter password: ")\n        if self.login_user(username, password):\n            print("Login successful.")\n            action = input("Do you want to add an item? (y/n): ")\n            if action.lower() == \'y\':\n                name = input("Item name: ")\n                value = int(input("Item value: "))\n                self.add_item(name, value)\n                print("Item added successfully!")\n            else:\n                print("No action taken.")\n        else:\n            print("Login failed. Goodbye.")\n\n    def close(self):\n        # Close the database connection\n        if self.conn:\n            self.conn.close()\n            self.log("Database connection closed.", "DEBUG")\n\nclass StringUtils:\n    @staticmethod\n    def trim(s: str) -> str:\n        return s.strip()\n\n    @staticmethod\n    def to_upper(s: str) -> str:\n        return s.upper()\n\n    @staticmethod\n    def to_lower(s: str) -> str:\n        return s.lower()\n\n    @staticmethod\n    def tokenize(s: str, delimiter: str = " ") -> list:\n        return s.split(delimiter)\n\n    @staticmethod\n    def is_alphanumeric(s: str) -> bool:\n        return s.isalnum()\n', 'Download_Data.py': "class GaTectorDataset(Dataset):\n    def __init__(self, root_dir, mat_file, input_shape, num_classes, train_mode,train):\n        super(GaTectorDataset, self).__init__()\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n        self.train = train\n        self.train_mode=train_mode\n\n        # GOO pickle\n        self.output_size = 64\n        self.input_size = 224\n        self.root_dir = root_dir\n        self.mat_file = mat_file\n        with open(mat_file, 'rb') as f:\n            self.data = pickle.load(f)\n            self.image_num = len(self.data)\n        self.transform = transforms.Compose([transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n                                            ])\n\n    def __len__(self):\n        return self.image_num\n\n    def __getitem__(self, index):\n        index = index % self.image_num\n\n        # GOO pickle\n        data = self.data[index]\n        image_path = data['filename']\n        image_path = os.path.join(self.root_dir, image_path)\n        image_path = image_path.replace('\\\\', '/')\n        gt_box_idx = data['gazeIdx']\n        # Goo gt_box\n        if self.train_mode==0:\n            gt_bboxes = np.copy(data['ann']['bboxes'])\n            gt_labels = np.copy(data['ann']['labels'])\n        if self.train_mode==1:\n            gt_bboxes = np.copy(data['ann']['bboxes']) / [640, 480, 640, 480] * [1920, 1080, 1920, 1080]\n            gt_labels = np.copy(data['ann']['labels'])\n\n        gt_labels = gt_labels[..., np.newaxis]\n        bbox = np.append(gt_bboxes, gt_labels, axis=1)\n        box = bbox.astype(np.int32)\n\n        gaze_gt_box = box[gt_box_idx]\n        gaze_gt_box = gaze_gt_box[np.newaxis, :]\n\n        # GOO\n        eye = [float(data['hx']) / 640, float(data['hy']) / 480]\n        gaze = [float(data['gaze_cx']) / 640, float(data['gaze_cy']) / 480]\n        img = Image.open(image_path)\n        img = img.convert('RGB')\n        width, height = img.size\n        gaze_x, gaze_y = gaze\n        eye_x, eye_y = eye\n\n        k = 0.1\n        x_min = (eye_x - 0.15) * width\n        y_min = (eye_y - 0.15) * height\n        x_max = (eye_x + 0.15) * width\n        y_max = (eye_y + 0.15) * height\n        if x_min < 0:\n            x_min = 0\n        if y_min < 0:\n            y_min = 0\n        if x_max < 0:\n            x_max = 0\n        if y_max < 0:\n            y_max = 0\n        x_min -= k * abs(x_max - x_min)\n        y_min -= k * abs(y_max - y_min)\n        x_max += k * abs(x_max - x_min)\n        y_max += k * abs(y_max - y_min)\n        x_min, y_min, x_max, y_max = map(float, [x_min, y_min, x_max, y_max])\n\n        if self.train:\n            # data augmentation\n            # Jitter (expansion-only) bounding box size\n            if np.random.random_sample() <= 0.5:\n                k = np.random.random_sample() * 0.2\n                x_min -= k * abs(x_max - x_min)\n                y_min -= k * abs(y_max - y_min)\n                x_max += k * abs(x_max - x_min)\n                y_max += k * abs(y_max - y_min)\n\n            # Random Crop\n            if np.random.random_sample() <= 0.5:\n                # Calculate the minimum valid range of the crop that doesn't exclude the face and the gaze target\n                crop_x_min = np.min([gaze_x * width, x_min, x_max])\n                crop_y_min = np.min([gaze_y * height, y_min, y_max])\n                crop_x_max = np.max([gaze_x * width, x_min, x_max])\n                crop_y_max = np.max([gaze_y * height, y_min, y_max])\n\n                # Randomly select a random top left corner\n                if crop_x_min >= 0:\n                    crop_x_min = np.random.uniform(0, crop_x_min)\n                if crop_y_min >= 0:\n                    crop_y_min = np.random.uniform(0, crop_y_min)\n\n                # Find the range of valid crop width and height starting from the (crop_x_min, crop_y_min)\n                crop_width_min = crop_x_max - crop_x_min\n                crop_height_min = crop_y_max - crop_y_min\n                crop_width_max = width - crop_x_min\n                crop_height_max = height - crop_y_min\n                # Randomly select a width and a height\n                crop_width = np.random.uniform(crop_width_min, crop_width_max)\n                crop_height = np.random.uniform(crop_height_min, crop_height_max)\n\n                # Crop it\n                img = TF.crop(img, crop_y_min, crop_x_min, crop_height, crop_width)\n\n                # Record the crop's (x, y) offset\n                offset_x, offset_y = crop_x_min, crop_y_min\n\n                # convert coordinates into the cropped frame\n                x_min, y_min, x_max, y_max = x_min - offset_x, y_min - offset_y, x_max - offset_x, y_max - offset_y\n                # if gaze_inside:\n                gaze_x, gaze_y = (gaze_x * width - offset_x) / float(crop_width), (gaze_y * height - offset_y) / float(crop_height)\n\n                width, height = crop_width, crop_height\n\n                box[:, [0, 2]] = box[:, [0, 2]] - crop_x_min\n                box[:, [1, 3]] = box[:, [1, 3]] - crop_y_min\n\n                # operate gt_box\n                gaze_gt_box[:, [0, 2]] = gaze_gt_box[:, [0, 2]] - crop_x_min\n                gaze_gt_box[:, [1, 3]] = gaze_gt_box[:, [1, 3]] - crop_y_min\n\n            # Random flip\n            if np.random.random_sample() <= 0.5:\n                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n                x_max_2 = width - x_min\n                x_min_2 = width - x_max\n                x_max = x_max_2\n                x_min = x_min_2\n                gaze_x = 1 - gaze_x\n                box[:, [0, 2]] = width - box[:, [2, 0]]\n\n            # Random color change\n            if np.random.random_sample() <= 0.5:\n                img = TF.adjust_brightness(img, brightness_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_contrast(img, contrast_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_saturation(img, saturation_factor=np.random.uniform(0, 1.5))\n\n            # Random color change\n            if np.random.random_sample() <= 0.5:\n                img = TF.adjust_brightness(img, brightness_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_contrast(img, contrast_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_saturation(img, saturation_factor=np.random.uniform(0, 1.5))\n\n        head_channel = gaze_imutils.get_head_box_channel(x_min, y_min, x_max, y_max, width, height,\n                                                        resolution=self.input_size, coordconv=False).unsqueeze(0)\n\n        # Crop the face\n        face = img.crop((int(x_min), int(y_min), int(x_max), int(y_max)))\n        face = face.resize((self.input_shape), Image.BICUBIC)\n        face = np.transpose(preprocess_input(np.array(face, dtype=np.float32)), (2, 0, 1))\n        face = torch.Tensor(face)\n        face = self.transform(face)\n        img = img.resize((self.input_shape), Image.BICUBIC)\n        img = np.transpose(preprocess_input(np.array(img, dtype=np.float32)), (2, 0, 1))\n        img = torch.Tensor(img)\n        img = self.transform(img)\n\n        # Bbox deal\n        box[:, [0, 2]] = box[:, [0, 2]] * self.input_size / width\n        box[:, [1, 3]] = box[:, [1, 3]] * self.input_size / height\n\n        # operate_gt_box\n        gaze_gt_box[:, [0, 2]] = gaze_gt_box[:, [0, 2]] * self.input_size / width\n        gaze_gt_box[:, [1, 3]] = gaze_gt_box[:, [1, 3]] * self.input_size / height\n\n        box[:, 0:2][box[:, 0:2] < 0] = 0\n        box[:, 2][box[:, 2] > self.input_size] = self.input_size\n        box[:, 3][box[:, 3] > self.input_size] = self.input_size\n        box_w = box[:, 2] - box[:, 0]\n        box_h = box[:, 3] - box[:, 1]\n        box = box[np.logical_and(box_w > 1, box_h > 1)]\n\n        box = np.array(box, dtype=np.float32)\n        if len(box) != 0:\n            box[:, [0, 2]] = box[:, [0, 2]] / self.input_shape[1]\n            box[:, [1, 3]] = box[:, [1, 3]] / self.input_shape[0]\n\n            box[:, 2:4] = box[:, 2:4] - box[:, 0:2]\n            box[:, 0:2] = box[:, 0:2] + box[:, 2:4] / 2\n\n        # generate the heatmap used for deconv prediction\n        gaze_heatmap = torch.zeros(self.output_size, self.output_size)\n        gaze_heatmap = gaze_imutils.draw_labelmap(gaze_heatmap, [gaze_x * self.output_size, gaze_y * self.output_size],\n                                                3,\n                                                type='Gaussian')\n        face = np.array(face, dtype=np.float32)\n        img = np.array(img, dtype=np.float32)\n        head_channel = np.array(head_channel, dtype=np.float32)\n        gaze_heatmap = np.array(gaze_heatmap, dtype=np.float32)\n\n        return img, box, face, head_channel, gaze_heatmap, eye, gaze, gaze_gt_box\n\n    def rand(self, a=0, b=1):\n        return np.random.rand() * (b - a) + a\n"}
2024-12-08 18:41:29,873 - __main__ - INFO - Task job: god_object
2024-12-08 18:43:21,299 - __main__ - ERROR - Error processing task message with correlation ID: 1234
2024-12-08 18:43:21,300 - __main__ - ERROR - 'list' object has no attribute 'items'
2024-12-08 18:43:38,365 - __main__ - INFO - Received task message
2024-12-08 18:43:38,366 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-08 18:43:38,367 - __main__ - INFO - Task type: detection
2024-12-08 18:43:38,367 - __main__ - INFO - Task data: {'Data_Processing.py': 'class OrderProcessor:\n    def __init__(self, validator, payment_gateway, notification_service):\n        self.validator = validator\n        self.payment_gateway = payment_gateway\n        self.notification_service = notification_service\n\n    def process_order(self, order):\n        if not self.validator.validate(order):\n            raise ValueError("Order is invalid")\n\n        payment_status = self.payment_gateway.charge(order)\n        if payment_status == "success":\n            self.notification_service.send_confirmation(order)\n        else:\n            self.notification_service.send_failure(order)\n', 'Data_Cleaning.py': 'class GodClass:\n    def __init__(self, config_path: str, db_path: str):\n        # Configuration\n        self.config = self._load_config(config_path)\n\n        # Database connection\n        self.db_path = db_path\n        self.conn = None\n\n        # Data cache\n        self.data_cache = []\n\n        # Logging settings\n        self.log_file = self.config.get("log_file", "app.log")\n        self.log_level = self.config.get("log_level", "INFO")\n\n        # State\n        self.user_is_logged_in = False\n        self.current_user = None\n\n    def _load_config(self, config_path: str):\n        # Load configuration from a JSON file\n        try:\n            with open(config_path, "r") as f:\n                config = json.load(f)\n        except FileNotFoundError:\n            config = {\n                "log_file": "app.log",\n                "log_level": "INFO",\n                "default_admin": "admin",\n                "default_password": "admin123"\n            }\n        return config\n\n    def log(self, message: str, level: str = "INFO"):\n        # Log a message to a file, ignoring log_level thresholds for simplicity\n        with open(self.log_file, "a") as f:\n            f.write(f"[{level}] {message}\\n")\n\n    def connect_db(self):\n        # Connect to the database\n        if self.conn is None:\n            self.conn = sqlite3.connect(self.db_path)\n            self.log("Connected to the database.", "DEBUG")\n\n    def create_tables(self):\n        # Create tables if they don\'t exist\n        self.connect_db()\n        cursor = self.conn.cursor()\n        self.conn.commit()\n        self.log("Tables ensured in database.", "DEBUG")\n\n    def add_user(self, username: str, password: str):\n        # Add a user to the database\n        self.connect_db()\n        try:\n            cursor = self.conn.cursor()\n            cursor.execute("INSERT INTO users (username, password) VALUES (?, ?)", (username, password))\n            self.conn.commit()\n            self.log(f"User added: {username}", "INFO")\n        except sqlite3.IntegrityError:\n            self.log(f"User {username} already exists.", "ERROR")\n\n    def login_user(self, username: str, password: str):\n        # Login a user\n        self.connect_db()\n        cursor = self.conn.cursor()\n        cursor.execute("SELECT * FROM users WHERE username = ? AND password = ?", (username, password))\n        user = cursor.fetchone()\n        if user:\n            self.user_is_logged_in = True\n            self.current_user = username\n            self.log(f"User {username} logged in.", "INFO")\n            return True\n        else:\n            self.log(f"Failed login attempt for user {username}.", "WARNING")\n            return False\n\n    def add_item(self, name: str, value: int):\n        # Add an item to the database\n        if not self.user_is_logged_in:\n            self.log("Attempted to add item without being logged in.", "ERROR")\n            return\n\n        self.connect_db()\n        cursor = self.conn.cursor()\n        cursor.execute("INSERT INTO items (name, value) VALUES (?, ?)", (name, value))\n        self.conn.commit()\n        self.log(f"Item added: {name} with value {value}", "INFO")\n\n    def load_data_into_cache(self):\n        # Load all items from the database into memory\n        self.connect_db()\n        cursor = self.conn.cursor()\n        cursor.execute("SELECT name, value FROM items")\n        self.data_cache = cursor.fetchall()\n        self.log("Data loaded into cache.", "DEBUG")\n\n    def process_data(self):\n        # Process data in memory (e.g., sorting by value)\n        if not self.data_cache:\n            self.log("Data cache is empty, cannot process.", "WARNING")\n            return\n        self.data_cache.sort(key=lambda x: x[1])\n        self.log("Data processed (sorted by value).", "INFO")\n\n    def user_interaction(self):\n        # Simulate user interaction via input/output\n        print("Welcome to the GodClass Application!")\n        username = input("Enter username: ")\n        password = input("Enter password: ")\n        if self.login_user(username, password):\n            print("Login successful.")\n            action = input("Do you want to add an item? (y/n): ")\n            if action.lower() == \'y\':\n                name = input("Item name: ")\n                value = int(input("Item value: "))\n                self.add_item(name, value)\n                print("Item added successfully!")\n            else:\n                print("No action taken.")\n        else:\n            print("Login failed. Goodbye.")\n\n    def close(self):\n        # Close the database connection\n        if self.conn:\n            self.conn.close()\n            self.log("Database connection closed.", "DEBUG")\n\nclass StringUtils:\n    @staticmethod\n    def trim(s: str) -> str:\n        return s.strip()\n\n    @staticmethod\n    def to_upper(s: str) -> str:\n        return s.upper()\n\n    @staticmethod\n    def to_lower(s: str) -> str:\n        return s.lower()\n\n    @staticmethod\n    def tokenize(s: str, delimiter: str = " ") -> list:\n        return s.split(delimiter)\n\n    @staticmethod\n    def is_alphanumeric(s: str) -> bool:\n        return s.isalnum()\n', 'Download_Data.py': "class GaTectorDataset(Dataset):\n    def __init__(self, root_dir, mat_file, input_shape, num_classes, train_mode,train):\n        super(GaTectorDataset, self).__init__()\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n        self.train = train\n        self.train_mode=train_mode\n\n        # GOO pickle\n        self.output_size = 64\n        self.input_size = 224\n        self.root_dir = root_dir\n        self.mat_file = mat_file\n        with open(mat_file, 'rb') as f:\n            self.data = pickle.load(f)\n            self.image_num = len(self.data)\n        self.transform = transforms.Compose([transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n                                            ])\n\n    def __len__(self):\n        return self.image_num\n\n    def __getitem__(self, index):\n        index = index % self.image_num\n\n        # GOO pickle\n        data = self.data[index]\n        image_path = data['filename']\n        image_path = os.path.join(self.root_dir, image_path)\n        image_path = image_path.replace('\\\\', '/')\n        gt_box_idx = data['gazeIdx']\n        # Goo gt_box\n        if self.train_mode==0:\n            gt_bboxes = np.copy(data['ann']['bboxes'])\n            gt_labels = np.copy(data['ann']['labels'])\n        if self.train_mode==1:\n            gt_bboxes = np.copy(data['ann']['bboxes']) / [640, 480, 640, 480] * [1920, 1080, 1920, 1080]\n            gt_labels = np.copy(data['ann']['labels'])\n\n        gt_labels = gt_labels[..., np.newaxis]\n        bbox = np.append(gt_bboxes, gt_labels, axis=1)\n        box = bbox.astype(np.int32)\n\n        gaze_gt_box = box[gt_box_idx]\n        gaze_gt_box = gaze_gt_box[np.newaxis, :]\n\n        # GOO\n        eye = [float(data['hx']) / 640, float(data['hy']) / 480]\n        gaze = [float(data['gaze_cx']) / 640, float(data['gaze_cy']) / 480]\n        img = Image.open(image_path)\n        img = img.convert('RGB')\n        width, height = img.size\n        gaze_x, gaze_y = gaze\n        eye_x, eye_y = eye\n\n        k = 0.1\n        x_min = (eye_x - 0.15) * width\n        y_min = (eye_y - 0.15) * height\n        x_max = (eye_x + 0.15) * width\n        y_max = (eye_y + 0.15) * height\n        if x_min < 0:\n            x_min = 0\n        if y_min < 0:\n            y_min = 0\n        if x_max < 0:\n            x_max = 0\n        if y_max < 0:\n            y_max = 0\n        x_min -= k * abs(x_max - x_min)\n        y_min -= k * abs(y_max - y_min)\n        x_max += k * abs(x_max - x_min)\n        y_max += k * abs(y_max - y_min)\n        x_min, y_min, x_max, y_max = map(float, [x_min, y_min, x_max, y_max])\n\n        if self.train:\n            # data augmentation\n            # Jitter (expansion-only) bounding box size\n            if np.random.random_sample() <= 0.5:\n                k = np.random.random_sample() * 0.2\n                x_min -= k * abs(x_max - x_min)\n                y_min -= k * abs(y_max - y_min)\n                x_max += k * abs(x_max - x_min)\n                y_max += k * abs(y_max - y_min)\n\n            # Random Crop\n            if np.random.random_sample() <= 0.5:\n                # Calculate the minimum valid range of the crop that doesn't exclude the face and the gaze target\n                crop_x_min = np.min([gaze_x * width, x_min, x_max])\n                crop_y_min = np.min([gaze_y * height, y_min, y_max])\n                crop_x_max = np.max([gaze_x * width, x_min, x_max])\n                crop_y_max = np.max([gaze_y * height, y_min, y_max])\n\n                # Randomly select a random top left corner\n                if crop_x_min >= 0:\n                    crop_x_min = np.random.uniform(0, crop_x_min)\n                if crop_y_min >= 0:\n                    crop_y_min = np.random.uniform(0, crop_y_min)\n\n                # Find the range of valid crop width and height starting from the (crop_x_min, crop_y_min)\n                crop_width_min = crop_x_max - crop_x_min\n                crop_height_min = crop_y_max - crop_y_min\n                crop_width_max = width - crop_x_min\n                crop_height_max = height - crop_y_min\n                # Randomly select a width and a height\n                crop_width = np.random.uniform(crop_width_min, crop_width_max)\n                crop_height = np.random.uniform(crop_height_min, crop_height_max)\n\n                # Crop it\n                img = TF.crop(img, crop_y_min, crop_x_min, crop_height, crop_width)\n\n                # Record the crop's (x, y) offset\n                offset_x, offset_y = crop_x_min, crop_y_min\n\n                # convert coordinates into the cropped frame\n                x_min, y_min, x_max, y_max = x_min - offset_x, y_min - offset_y, x_max - offset_x, y_max - offset_y\n                # if gaze_inside:\n                gaze_x, gaze_y = (gaze_x * width - offset_x) / float(crop_width), (gaze_y * height - offset_y) / float(crop_height)\n\n                width, height = crop_width, crop_height\n\n                box[:, [0, 2]] = box[:, [0, 2]] - crop_x_min\n                box[:, [1, 3]] = box[:, [1, 3]] - crop_y_min\n\n                # operate gt_box\n                gaze_gt_box[:, [0, 2]] = gaze_gt_box[:, [0, 2]] - crop_x_min\n                gaze_gt_box[:, [1, 3]] = gaze_gt_box[:, [1, 3]] - crop_y_min\n\n            # Random flip\n            if np.random.random_sample() <= 0.5:\n                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n                x_max_2 = width - x_min\n                x_min_2 = width - x_max\n                x_max = x_max_2\n                x_min = x_min_2\n                gaze_x = 1 - gaze_x\n                box[:, [0, 2]] = width - box[:, [2, 0]]\n\n            # Random color change\n            if np.random.random_sample() <= 0.5:\n                img = TF.adjust_brightness(img, brightness_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_contrast(img, contrast_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_saturation(img, saturation_factor=np.random.uniform(0, 1.5))\n\n            # Random color change\n            if np.random.random_sample() <= 0.5:\n                img = TF.adjust_brightness(img, brightness_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_contrast(img, contrast_factor=np.random.uniform(0.5, 1.5))\n                img = TF.adjust_saturation(img, saturation_factor=np.random.uniform(0, 1.5))\n\n        head_channel = gaze_imutils.get_head_box_channel(x_min, y_min, x_max, y_max, width, height,\n                                                        resolution=self.input_size, coordconv=False).unsqueeze(0)\n\n        # Crop the face\n        face = img.crop((int(x_min), int(y_min), int(x_max), int(y_max)))\n        face = face.resize((self.input_shape), Image.BICUBIC)\n        face = np.transpose(preprocess_input(np.array(face, dtype=np.float32)), (2, 0, 1))\n        face = torch.Tensor(face)\n        face = self.transform(face)\n        img = img.resize((self.input_shape), Image.BICUBIC)\n        img = np.transpose(preprocess_input(np.array(img, dtype=np.float32)), (2, 0, 1))\n        img = torch.Tensor(img)\n        img = self.transform(img)\n\n        # Bbox deal\n        box[:, [0, 2]] = box[:, [0, 2]] * self.input_size / width\n        box[:, [1, 3]] = box[:, [1, 3]] * self.input_size / height\n\n        # operate_gt_box\n        gaze_gt_box[:, [0, 2]] = gaze_gt_box[:, [0, 2]] * self.input_size / width\n        gaze_gt_box[:, [1, 3]] = gaze_gt_box[:, [1, 3]] * self.input_size / height\n\n        box[:, 0:2][box[:, 0:2] < 0] = 0\n        box[:, 2][box[:, 2] > self.input_size] = self.input_size\n        box[:, 3][box[:, 3] > self.input_size] = self.input_size\n        box_w = box[:, 2] - box[:, 0]\n        box_h = box[:, 3] - box[:, 1]\n        box = box[np.logical_and(box_w > 1, box_h > 1)]\n\n        box = np.array(box, dtype=np.float32)\n        if len(box) != 0:\n            box[:, [0, 2]] = box[:, [0, 2]] / self.input_shape[1]\n            box[:, [1, 3]] = box[:, [1, 3]] / self.input_shape[0]\n\n            box[:, 2:4] = box[:, 2:4] - box[:, 0:2]\n            box[:, 0:2] = box[:, 0:2] + box[:, 2:4] / 2\n\n        # generate the heatmap used for deconv prediction\n        gaze_heatmap = torch.zeros(self.output_size, self.output_size)\n        gaze_heatmap = gaze_imutils.draw_labelmap(gaze_heatmap, [gaze_x * self.output_size, gaze_y * self.output_size],\n                                                3,\n                                                type='Gaussian')\n        face = np.array(face, dtype=np.float32)\n        img = np.array(img, dtype=np.float32)\n        head_channel = np.array(head_channel, dtype=np.float32)\n        gaze_heatmap = np.array(gaze_heatmap, dtype=np.float32)\n\n        return img, box, face, head_channel, gaze_heatmap, eye, gaze, gaze_gt_box\n\n    def rand(self, a=0, b=1):\n        return np.random.rand() * (b - a) + a\n"}
2024-12-08 18:43:38,370 - __main__ - INFO - Task job: god_object
2024-12-08 18:45:42,057 - __main__ - ERROR - Error processing task message with correlation ID: 1234
2024-12-08 18:45:42,058 - __main__ - ERROR - 'list' object has no attribute 'items'
2024-12-08 18:51:44,541 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-08 18:51:44,568 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-08 18:51:44,623 - llm_processor - INFO - Models directory: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 18:51:44,624 - llm_processor - INFO - Loading model pipeline: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 18:51:45,203 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-12-08 18:51:48,773 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
2024-12-08 18:51:48,775 - llm_processor - INFO - Model pipeline loaded: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 18:51:48,775 - __main__ - INFO - Initializing knowledge base for detection
2024-12-08 18:51:48,776 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-08 18:51:48,776 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-08 18:51:48,778 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-08 18:51:48,779 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-08 18:51:54,946 - __main__ - INFO - Received task message
2024-12-08 18:51:54,947 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-08 18:51:54,947 - __main__ - INFO - Task type: detection
2024-12-08 18:51:54,947 - __main__ - INFO - Task job: god_object
2024-12-08 18:51:58,401 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-08 18:51:58,402 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-08 18:53:47,057 - llm_processor - INFO - Processed result: File: {Data_Cleaning.py}  
Detected: {GodClass}

File: {Data_Cleaning.py}  
Detected: {StringUtils}

File: {Download_Data.py}  
Detected: {GaTectorDataset}
2024-12-08 19:00:50,745 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-08 19:00:50,768 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-08 19:00:50,821 - llm_processor - INFO - Models directory: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 19:00:50,822 - llm_processor - INFO - Loading model pipeline: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 19:00:51,401 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-12-08 19:00:54,496 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
2024-12-08 19:00:54,497 - llm_processor - INFO - Model pipeline loaded: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 19:00:54,498 - __main__ - INFO - Initializing knowledge base for detection
2024-12-08 19:00:54,498 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-08 19:00:54,499 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-08 19:00:54,499 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-08 19:00:54,500 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-08 19:01:12,683 - __main__ - INFO - Received task message
2024-12-08 19:01:12,683 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-08 19:01:12,684 - __main__ - INFO - Task type: detection
2024-12-08 19:01:12,684 - __main__ - INFO - Task job: god_object
2024-12-08 19:01:12,939 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-08 19:01:12,940 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-08 19:09:03,878 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-08 19:09:03,886 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-08 19:09:03,939 - llm_processor - INFO - Models directory: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 19:09:03,939 - llm_processor - INFO - Loading model pipeline: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 19:09:04,416 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-12-08 19:09:06,886 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
2024-12-08 19:09:06,887 - llm_processor - INFO - Model pipeline loaded: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 19:09:06,888 - __main__ - INFO - Initializing knowledge base for detection
2024-12-08 19:09:06,888 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-08 19:09:06,888 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-08 19:09:06,889 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-08 19:09:06,889 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-08 19:09:13,061 - __main__ - INFO - Received task message
2024-12-08 19:09:13,062 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-08 19:09:13,063 - __main__ - INFO - Task type: detection
2024-12-08 19:09:13,063 - __main__ - INFO - Task job: god_object
2024-12-08 19:09:13,265 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-08 19:09:13,266 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-08 19:14:00,041 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-08 19:14:00,069 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-08 19:14:00,117 - llm_processor - INFO - Models directory: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 19:14:00,117 - llm_processor - INFO - Loading model pipeline: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 19:14:00,558 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-12-08 19:14:03,094 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
2024-12-08 19:14:03,095 - llm_processor - INFO - Model pipeline loaded: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 19:14:03,096 - __main__ - INFO - Initializing knowledge base for detection
2024-12-08 19:14:03,096 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-08 19:14:03,096 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-08 19:14:03,097 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-08 19:14:03,098 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-08 19:14:33,310 - __main__ - INFO - Received task message
2024-12-08 19:14:33,310 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-08 19:14:33,311 - __main__ - INFO - Task type: detection
2024-12-08 19:14:33,311 - __main__ - INFO - Task job: god_object
2024-12-08 19:14:46,747 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-08 19:14:46,748 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-08 19:16:44,507 - llm_processor - INFO - Processed result: File: Data_Cleaning.py
Detected: GodClass

File: Data_Cleaning.py
Detected: GodClass

File: Data_Cleaning.py
Detected: GodClass

File: Data_Cleaning.py
Detected: GodClass

File: Data_Cleaning.py
Detected: GodClass
2024-12-08 19:17:03,992 - __main__ - INFO - Received task message
2024-12-08 19:17:03,993 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-08 19:17:03,993 - __main__ - INFO - Task type: detection
2024-12-08 19:17:03,994 - __main__ - INFO - Task job: god_object
2024-12-08 19:17:06,863 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-08 19:17:06,863 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-08 19:19:52,213 - llm_processor - INFO - Processed result: File: {Data_Cleaning.py}
Detected: {GodClass}

File: {Data_Cleaning.py}
Detected: {StringUtils}

File: {Data_Cleaning.py}
Detected: {StringUtils}

File: {Data_Cleaning.py}
Detected: {StringUtils}

File: {Data_Cleaning.py}
Detected: {StringUtils}

File: {Data_Cleaning.py}
Detected: {StringUtils}

File: {Download_Data.py}
Detected: {GaTectorDataset}
2024-12-08 19:51:59,063 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-08 19:51:59,087 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-08 19:51:59,139 - llm_processor - INFO - Models directory: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 19:51:59,140 - llm_processor - INFO - Loading local model pipeline: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 19:51:59,596 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-12-08 19:52:02,072 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
2024-12-08 19:52:02,074 - llm_processor - INFO - Local model pipeline loaded: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 19:52:02,074 - __main__ - INFO - Initializing knowledge base for detection
2024-12-08 19:52:02,075 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-08 19:52:02,075 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-08 19:52:02,076 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-08 19:52:02,077 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-08 19:52:08,252 - __main__ - INFO - Received task message
2024-12-08 19:52:08,252 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-08 19:52:08,253 - __main__ - INFO - Task type: detection
2024-12-08 19:52:08,253 - __main__ - INFO - Task job: god_object
2024-12-08 19:52:08,451 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-08 19:52:08,451 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-08 19:57:29,877 - llm_processor - INFO - Processed result: File: Data_Cleaning.py  
Detected: GodClass  
Issue: The class takes on multiple unrelated responsibilities, including configuration, database connection, logging, data caching, user management, and UI interaction, violating the Single Responsibility Principle.

File: Data_Cleaning.py  
Detected: StringUtils  
Issue: The class is not flagged as a large class as it is cohesive and single-purpose, focusing on string manipulation operations.

File: Download_Data.py  
Detected: GaTectorDataset  
Issue: The class takes on multiple unrelated responsibilities, including data loading, data augmentation, and image processing, violating the Single Responsibility Principle.
2024-12-08 22:44:10,173 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-08 22:44:10,200 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-08 22:44:10,248 - llm_processor - INFO - Models directory: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 22:44:10,248 - llm_processor - INFO - Loading local model pipeline: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 22:44:10,787 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-12-08 22:44:14,890 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
2024-12-08 22:44:14,891 - llm_processor - INFO - Local model pipeline loaded: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 22:44:14,891 - __main__ - INFO - Initializing knowledge base for detection
2024-12-08 22:44:14,892 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-08 22:44:14,892 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-08 22:44:14,899 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-08 22:44:14,899 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-08 22:45:16,142 - __main__ - INFO - Received task message
2024-12-08 22:45:16,144 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-08 22:45:16,145 - __main__ - INFO - Task type: detection
2024-12-08 22:45:16,146 - __main__ - INFO - Task job: long_function
2024-12-08 22:45:16,558 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-08 22:45:16,558 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-08 22:51:58,750 - llm_processor - INFO - Processed result: File: OrderProcessing.py  
Detected: OrderProcessor.OrderProcessor.process_order  
Issue: Function spans 35 lines and performs multiple tasks such as validating order items, deducting stock, processing payment, and notifying the customer.

File: UserDataProcessing.py  
Detected: process_user_data  
Issue: Function spans 23 lines and performs multiple tasks such as reading data from a file, validating the data, storing it in a database, and sending confirmation emails.

File: Logging.py  
Detected: log_message  
Issue: Function spans 16 lines and performs multiple tasks such as logging messages at different levels.

File: DatabaseUtils.py  
Detected: connect_to_database  
Issue: Function spans 8 lines and performs a single task of establishing a connection to the database.

File: DatabaseUtils.py  
Detected: add_user_to_database  
Issue: Function spans 12 lines and performs a single task of adding a user to the database.

File: OrderProcessing.py  
Detected: OrderProcessor.OrderProcessor.process_order  
Issue: Function contains deeply nested if-else statements and performs both data cleaning and visualization tasks, which should be separated into smaller functions.

File: UserDataProcessing.py  
Detected: process_user_data  
Issue: Function contains multiple tasks that should be split into
2024-12-08 22:57:43,359 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-08 22:57:43,367 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-08 22:57:43,415 - llm_processor - INFO - Models directory: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 22:57:43,415 - llm_processor - INFO - Loading local model pipeline: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 22:57:43,885 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-12-08 22:57:46,206 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
2024-12-08 22:57:46,207 - llm_processor - INFO - Local model pipeline loaded: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 22:57:46,208 - __main__ - INFO - Initializing knowledge base for detection
2024-12-08 22:57:46,208 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-08 22:57:46,209 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-08 22:57:46,210 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-08 22:57:46,211 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-08 22:57:57,066 - __main__ - INFO - Received task message
2024-12-08 22:57:57,068 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-08 22:57:57,070 - __main__ - INFO - Task type: detection
2024-12-08 22:57:57,071 - __main__ - INFO - Task job: long_function
2024-12-08 22:57:57,308 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-08 22:57:57,310 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-08 23:08:59,298 - llm_processor - INFO - Processed result: File:OrderProcessing.py  
Detected:process_order  
Issue:Performs multiple tasks such as inventory validation, payment processing, and notification sending, and is too long to be easily maintained.

File:UserDataProcessing.py  
Detected:process_user_data  
Issue:Performs multiple tasks such as file reading, data validation, database storage, and email sending, and is too long to be easily maintained.

File:OrderProcessing.py  
Detected:process_order  
Issue:Has complicated logic due to excessive nesting and multiple conditional statements.

File:UserDataProcessing.py  
Detected:process_user_data  
Issue:Has complicated logic due to excessive nesting and multiple conditional statements.

File:OrderProcessing.py  
Detected:process_order  
Issue:Exceeds a reasonable line threshold with more than 30 lines of substantive code.

File:UserDataProcessing.py  
Detected:process_user_data  
Issue:Exceeds a reasonable line threshold with more than 30 lines of substantive code.

File:Logging.py  
Detected:log_message  
Issue:Performs multiple tasks such as logging at different levels and handling unknown log levels, and is too complex to be easily maintained.

File:DatabaseUtils.py  
Detected:connect_to_database  
Issue:Performs multiple tasks such
2024-12-08 23:11:16,236 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-08 23:11:16,260 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-08 23:11:16,313 - llm_processor - INFO - Models directory: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 23:11:16,314 - llm_processor - INFO - Loading local model pipeline: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 23:11:16,790 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-12-08 23:11:19,159 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
2024-12-08 23:11:19,160 - llm_processor - INFO - Local model pipeline loaded: E:\FYP\codenexus_microservices\codenexus_microservices\llm-service\models\LLAMA-3.1_8_I
2024-12-08 23:11:19,161 - __main__ - INFO - Initializing knowledge base for detection
2024-12-08 23:11:19,161 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-08 23:11:19,161 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-08 23:11:19,162 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-08 23:11:19,163 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-08 23:11:26,736 - __main__ - INFO - Received task message
2024-12-08 23:11:26,736 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-08 23:11:26,736 - __main__ - INFO - Task type: detection
2024-12-08 23:11:26,737 - __main__ - INFO - Task job: long_function
2024-12-08 23:11:26,980 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-08 23:11:26,980 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-08 23:22:46,285 - llm_processor - INFO - Processed result: File:OrderProcessing.py
Detected:process_order
Issue:Performs multiple tasks and is too long to be easily maintained.

File:UserDataProcessing.py
Detected:process_user_data
Issue:Performs multiple tasks and is too long to be easily maintained.

File:OrderProcessing.py
Detected:process_order
Issue:Has complicated logic due to excessive nesting and multiple responsibilities.

File:UserDataProcessing.py
Detected:process_user_data
Issue:Has complicated logic due to excessive nesting and multiple responsibilities.

File:Logging.py
Detected:log_message
Issue:Exceeds a reasonable line threshold and performs multiple unrelated tasks.

File:DatabaseUtils.py
Detected:connect_to_database
Issue:Exceeds a reasonable line threshold and performs multiple unrelated tasks.

File:DatabaseUtils.py
Detected:add_user_to_database
Issue:Exceeds a reasonable line threshold and performs multiple unrelated tasks.

File:OrderProcessing.py
Detected:process_order
Issue:Has multiple responsibilities such as inventory management, payment processing, and notification sending.

File:UserDataProcessing.py
Detected:process_user_data
Issue:Has multiple responsibilities such as data validation, database storage, and email sending.
2024-12-09 13:47:11,442 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-09 13:47:11,475 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-09 13:47:11,527 - llm_processor - INFO - Loading model pipeline from Hugging Face API: meta-llama/Llama-3.1-70B-Instruct
2024-12-09 13:47:11,527 - llm_processor - INFO - Model Client loaded from Hugging Face Inference API
2024-12-09 13:47:11,527 - __main__ - INFO - Initializing knowledge base for detection
2024-12-09 13:47:11,528 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-09 13:47:11,528 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-09 13:47:11,535 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-09 13:47:11,536 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-09 13:47:23,734 - __main__ - INFO - Received task message
2024-12-09 13:47:23,735 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 13:47:23,735 - __main__ - INFO - Task type: detection
2024-12-09 13:47:23,736 - __main__ - INFO - Task job: long_function
2024-12-09 13:47:23,928 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 13:47:23,928 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 13:49:12,753 - __main__ - INFO - Received task message
2024-12-09 13:49:12,754 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 13:49:12,754 - __main__ - INFO - Task type: detection
2024-12-09 13:49:12,755 - __main__ - INFO - Task job: long_function
2024-12-09 13:49:12,757 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 13:49:12,757 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 13:49:33,342 - __main__ - INFO - Received task message
2024-12-09 13:49:33,342 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 13:49:33,343 - __main__ - INFO - Task type: detection
2024-12-09 13:49:33,343 - __main__ - INFO - Task job: long_function
2024-12-09 13:49:33,345 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 13:49:33,346 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 13:49:50,522 - __main__ - INFO - Received task message
2024-12-09 13:49:50,523 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 13:49:50,523 - __main__ - INFO - Task type: detection
2024-12-09 13:49:50,523 - __main__ - INFO - Task job: long_function
2024-12-09 13:49:50,525 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 13:49:50,525 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 13:50:39,845 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-09 13:50:39,873 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-09 13:50:39,926 - llm_processor - INFO - Loading model pipeline from Hugging Face API: meta-llama/Llama-3.1-70B-Instruct
2024-12-09 13:50:39,927 - llm_processor - INFO - Model Client loaded from Hugging Face Inference API
2024-12-09 13:50:39,927 - __main__ - INFO - Initializing knowledge base for detection
2024-12-09 13:50:39,927 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-09 13:50:39,928 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-09 13:50:39,929 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-09 13:50:39,929 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-09 13:50:48,182 - __main__ - INFO - Received task message
2024-12-09 13:50:48,183 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 13:50:48,183 - __main__ - INFO - Task type: detection
2024-12-09 13:50:48,183 - __main__ - INFO - Task job: long_function
2024-12-09 13:50:48,252 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 13:50:48,253 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 13:52:35,426 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-09 13:52:35,455 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-09 13:52:35,504 - llm_processor - INFO - Loading model pipeline from Hugging Face API: meta-llama/Llama-3.1-70B-Instruct
2024-12-09 13:52:35,505 - llm_processor - INFO - Model Client loaded from Hugging Face Inference API
2024-12-09 13:52:35,505 - __main__ - INFO - Initializing knowledge base for detection
2024-12-09 13:52:35,505 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-09 13:52:35,505 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-09 13:52:35,506 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-09 13:52:35,507 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-09 13:52:40,235 - __main__ - INFO - Received task message
2024-12-09 13:52:40,236 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 13:52:40,236 - __main__ - INFO - Task type: detection
2024-12-09 13:52:40,237 - __main__ - INFO - Task job: long_function
2024-12-09 13:52:40,311 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 13:52:40,311 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 13:54:44,274 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-09 13:54:44,295 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-09 13:54:44,341 - llm_processor - INFO - Loading model pipeline from Hugging Face API: meta-llama/Llama-3.1-70B-Instruct
2024-12-09 13:54:44,342 - llm_processor - INFO - Model Client loaded from Hugging Face Inference API
2024-12-09 13:54:44,342 - __main__ - INFO - Initializing knowledge base for detection
2024-12-09 13:54:44,342 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-09 13:54:44,342 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-09 13:54:44,343 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-09 13:54:44,343 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-09 13:54:52,097 - __main__ - INFO - Received task message
2024-12-09 13:54:52,097 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 13:54:52,098 - __main__ - INFO - Task type: detection
2024-12-09 13:54:52,098 - __main__ - INFO - Task job: god_object
2024-12-09 13:54:52,181 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 13:54:52,182 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 13:55:44,228 - __main__ - INFO - Received task message
2024-12-09 13:55:44,228 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 13:55:44,229 - __main__ - INFO - Task type: detection
2024-12-09 13:55:44,229 - __main__ - INFO - Task job: god_object
2024-12-09 13:55:44,240 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 13:55:44,240 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 13:57:10,432 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-09 13:57:10,464 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-09 13:57:10,510 - llm_processor - INFO - Loading model pipeline from Hugging Face API: meta-llama/Llama-3.1-70B-Instruct
2024-12-09 13:57:10,510 - llm_processor - INFO - Model Client loaded from Hugging Face Inference API
2024-12-09 13:57:10,511 - __main__ - INFO - Initializing knowledge base for detection
2024-12-09 13:57:10,511 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-09 13:57:10,511 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-09 13:57:10,512 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-09 13:57:10,513 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-09 13:57:20,125 - __main__ - INFO - Received task message
2024-12-09 13:57:20,125 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 13:57:20,126 - __main__ - INFO - Task type: detection
2024-12-09 13:57:20,126 - __main__ - INFO - Task job: god_object
2024-12-09 13:57:20,206 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 13:57:20,207 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 13:57:29,282 - llm_processor - INFO - Processed result: File:{Data_Cleaning.py}
Detected:{GodClass}

Explanation:
The GodClass is a large class that takes on multiple unrelated responsibilities such as configuration, database connection, logging, data processing, and user interaction. This class handles more than one clear domain concern, making it a God Object code smell.
2024-12-09 13:57:39,917 - __main__ - INFO - Received task message
2024-12-09 13:57:39,917 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 13:57:39,918 - __main__ - INFO - Task type: detection
2024-12-09 13:57:39,918 - __main__ - INFO - Task job: long_function
2024-12-09 13:57:39,971 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 13:57:39,972 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 13:57:40,218 - llm_processor - INFO - Processed result: File:OrderProcessing.py  
Detected:OrderProcessor.process_order  
Issue:Performs multiple unrelated tasks, including order validation, inventory management, payment processing, notification, and logging.

File:UserDataProcessing.py  
Detected:process_user_data  
Issue:Performs multiple unrelated tasks, including file reading, data validation, database operations, and email sending.
2024-12-09 14:01:44,556 - __main__ - INFO - Received task message
2024-12-09 14:01:44,557 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 14:01:44,564 - __main__ - INFO - Task type: detection
2024-12-09 14:01:44,564 - __main__ - INFO - Task job: god_object
2024-12-09 14:01:44,576 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 14:01:44,577 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 14:01:44,825 - llm_processor - INFO - Processed result: File:{Data_Cleaning.py}
Detected:{GodClass}

Explanation:
The GodClass is a large class that takes on multiple unrelated responsibilities such as configuration, database connection, logging, data processing, and user interaction. This class handles more than one clear domain concern, making it a God Object code smell.
2024-12-09 14:02:02,605 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-09 14:02:02,613 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-09 14:02:02,659 - llm_processor - INFO - Loading model pipeline from Hugging Face API: meta-llama/Llama-3.1-70B-Instruct
2024-12-09 14:02:02,660 - llm_processor - INFO - Model Client loaded from Hugging Face Inference API
2024-12-09 14:02:02,660 - __main__ - INFO - Initializing knowledge base for detection
2024-12-09 14:02:02,660 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-09 14:02:02,661 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-09 14:02:02,662 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-09 14:02:02,662 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-09 14:02:09,426 - __main__ - INFO - Received task message
2024-12-09 14:02:09,427 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 14:02:09,427 - __main__ - INFO - Task type: detection
2024-12-09 14:02:09,427 - __main__ - INFO - Task job: god_object
2024-12-09 14:02:09,507 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 14:02:09,508 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 14:02:19,651 - llm_processor - INFO - Processed result: File:Data_Cleaning.py  
Detected:GodClass  
Issue:This class handles unrelated responsibilities such as configuration loading, database connection, data caching, logging, user login, item addition, data processing, and user interaction. It is far from cohesive and handles multiple domain concerns, making it a God Object.
2024-12-09 14:02:25,201 - __main__ - INFO - Received task message
2024-12-09 14:02:25,202 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 14:02:25,203 - __main__ - INFO - Task type: detection
2024-12-09 14:02:25,203 - __main__ - INFO - Task job: long_function
2024-12-09 14:02:25,255 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 14:02:25,256 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 14:02:25,489 - llm_processor - INFO - Processed result: File:OrderProcessing.py  
Detected:OrderProcessor.process_order  
Issue:Performs multiple unrelated tasks, including order validation, inventory management, payment processing, notification, and logging.

File:UserDataProcessing.py  
Detected:process_user_data  
Issue:Performs multiple unrelated tasks, including file reading, data validation, database operations, and email sending.
2024-12-09 14:25:33,894 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-09 14:25:33,921 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-09 14:25:33,975 - llm_processor - INFO - Loading model pipeline from Hugging Face API: meta-llama/Llama-3.1-70B-Instruct
2024-12-09 14:25:33,976 - llm_processor - INFO - Model Client loaded from Hugging Face Inference API
2024-12-09 14:25:33,976 - __main__ - INFO - Initializing knowledge base for detection
2024-12-09 14:25:33,977 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-09 14:25:33,977 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-09 14:25:33,978 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-09 14:25:33,979 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-09 14:25:47,635 - __main__ - INFO - Received task message
2024-12-09 14:25:47,635 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 14:25:47,636 - __main__ - INFO - Task type: detection
2024-12-09 14:25:47,636 - __main__ - INFO - Task job: god_object
2024-12-09 14:25:47,813 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 14:25:47,814 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 14:27:55,545 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-09 14:27:55,573 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-09 14:27:55,626 - llm_processor - INFO - Loading model pipeline from Hugging Face API: meta-llama/Llama-3.1-70B-Instruct
2024-12-09 14:27:55,626 - llm_processor - INFO - Model Client loaded from Hugging Face Inference API
2024-12-09 14:27:55,627 - __main__ - INFO - Initializing knowledge base for detection
2024-12-09 14:27:55,627 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-09 14:27:55,627 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-09 14:27:55,628 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-09 14:27:55,629 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-09 14:28:01,789 - __main__ - INFO - Received task message
2024-12-09 14:28:01,790 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 14:28:01,791 - __main__ - INFO - Task type: detection
2024-12-09 14:28:01,791 - __main__ - INFO - Task job: god_object
2024-12-09 14:28:01,799 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 14:28:01,799 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 14:28:02,842 - llm_processor - INFO - Processed result: File:Data_Cleaning.py  
Detected:GodClass  
Issue:This class handles unrelated responsibilities such as configuration loading, database connection, data caching, logging, user login, item addition, data processing, and user interaction. It is far from cohesive and handles multiple domain concerns, making it a God Object.
2024-12-09 14:29:37,019 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-09 14:29:37,027 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-09 14:29:37,075 - llm_processor - INFO - Loading model pipeline from Hugging Face API: meta-llama/Llama-3.1-70B-Instruct
2024-12-09 14:29:37,075 - llm_processor - INFO - Model Client loaded from Hugging Face Inference API
2024-12-09 14:29:37,076 - __main__ - INFO - Initializing knowledge base for detection
2024-12-09 14:29:37,076 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-09 14:29:37,076 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-09 14:29:37,077 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-09 14:29:37,077 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-09 14:29:45,213 - __main__ - INFO - Received task message
2024-12-09 14:29:45,214 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 14:29:45,214 - __main__ - INFO - Task type: detection
2024-12-09 14:29:45,215 - __main__ - INFO - Task job: god_object
2024-12-09 14:29:45,404 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 14:29:45,404 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 14:30:33,769 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-09 14:30:33,798 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-09 14:30:33,844 - llm_processor - INFO - Loading model pipeline from Hugging Face API: meta-llama/Llama-3.1-70B-Instruct
2024-12-09 14:30:33,845 - llm_processor - INFO - Model Client loaded from Hugging Face Inference API
2024-12-09 14:30:33,845 - __main__ - INFO - Initializing knowledge base for detection
2024-12-09 14:30:33,846 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-09 14:30:33,846 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-09 14:30:33,847 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-09 14:30:33,847 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-09 14:30:42,141 - __main__ - INFO - Received task message
2024-12-09 14:30:42,142 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 14:30:42,142 - __main__ - INFO - Task type: detection
2024-12-09 14:30:42,143 - __main__ - INFO - Task job: god_object
2024-12-09 14:30:42,150 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 14:30:42,151 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 14:30:43,076 - llm_processor - INFO - Processed result: File:Data_Cleaning.py  
Detected:GodClass  
Issue:This class handles unrelated responsibilities such as configuration loading, database connection, data caching, logging, user login, item addition, data processing, and user interaction. It is far from cohesive and handles multiple domain concerns, making it a God Object.
2024-12-09 14:30:50,214 - __main__ - INFO - Received task message
2024-12-09 14:30:50,215 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 14:30:50,215 - __main__ - INFO - Task type: detection
2024-12-09 14:30:50,216 - __main__ - INFO - Task job: god_object
2024-12-09 14:30:50,227 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 14:30:50,227 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 14:30:50,457 - llm_processor - INFO - Processed result: File:Data_Cleaning.py  
Detected:GodClass  
Issue:This class handles unrelated responsibilities such as configuration loading, database connection, data caching, logging, user login, item addition, data processing, and user interaction. It is far from cohesive and handles multiple domain concerns, making it a God Object.
2024-12-09 14:31:00,881 - __main__ - INFO - Received task message
2024-12-09 14:31:00,881 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 14:31:00,882 - __main__ - INFO - Task type: detection
2024-12-09 14:31:00,883 - __main__ - INFO - Task job: long_function
2024-12-09 14:31:00,884 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 14:31:00,885 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 14:31:05,766 - llm_processor - INFO - Processed result: File:OrderProcessing.py
Detected:OrderProcessor.process_order
Issue:Performs multiple tasks such as validating order items, deducting stock, processing payment, and notifying the customer, making the function overly complex and hard to maintain.

File:UserDataProcessing.py
Detected:process_user_data
Issue:Performs multiple unrelated tasks such as reading data from a file, validating the data, storing it in a database, and sending confirmation emails, making the function excessively long and hard to maintain.
2024-12-09 14:31:25,391 - __main__ - INFO - Received task message
2024-12-09 14:31:25,391 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 14:31:25,392 - __main__ - INFO - Task type: detection
2024-12-09 14:31:25,392 - __main__ - INFO - Task job: feature_envy
2024-12-09 14:31:25,393 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 14:31:25,394 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 14:31:38,905 - llm_processor - INFO - Processed result: File: OrderProcessing.py  
Detected: OrderProcessor.process_order  
Issue: The method excessively interacts with the `payment_service` and `inventory` objects, accessing their methods (`charge`, `check_stock`, and `update_stock`) to process an order, instead of delegating responsibilities to these classes.

File: UserManagement.py  
Detected: UserManager.add_user  
Issue: The method relies heavily on the `db` object's `add_user` method, which might be more appropriate as part of the `Database` class.

File: UserManagement.py  
Detected: UserManager.login  
Issue: The method excessively interacts with the `db` object, accessing its `validate_credentials` method to validate login credentials, instead of delegating this responsibility to the `Database` class.
2024-12-09 14:32:05,852 - __main__ - INFO - Received task message
2024-12-09 14:32:05,853 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 14:32:05,853 - __main__ - INFO - Task type: detection
2024-12-09 14:32:05,854 - __main__ - INFO - Task job: inappropriate_intimacy
2024-12-09 14:32:05,855 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 14:32:05,855 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 14:32:14,898 - llm_processor - INFO - Processed result: File:{OrderProcessor.py}
Detected:{OrderProcessor.process_order_direct_access}
Issue: The method directly accesses and modifies private attribute `_stock` of the `Inventory` class, bypassing its public interface and tightly coupling the two classes.

File:{CustomerAndNotification.py}
Detected:{NotificationService.send_direct_access}
Issue: The method directly accesses private attributes `name` and `email` of the `Customer` class, bypassing its public interface and tightly coupling the two classes.
2024-12-09 14:32:46,072 - __main__ - INFO - Received task message
2024-12-09 14:32:46,073 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 14:32:46,073 - __main__ - INFO - Task type: detection
2024-12-09 14:32:46,074 - __main__ - INFO - Task job: excessive_flags
2024-12-09 14:32:46,075 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 14:32:46,075 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 14:33:19,199 - llm_processor - INFO - Processed result: File: {OrderProcessor.py}
Detected: {OrderProcessor.OrderProcessor.process_order}
Issue: The function takes multiple flags (`is_priority`, `is_gift`, `requires_signature`) that significantly alter its behavior, leading to multiple unrelated tasks being handled within the same function.

File: {OrderProcessor.py}
Detected: {process_refund}
Issue: The function uses two flags (`is_partial`, `is_expedited`) to control its behavior, resulting in distinct execution paths that could be split into separate, more focused functions.

File: {PaymentHandler.py}
Detected: {PaymentHandler.PaymentHandler.handle_payment}
Issue: The function accepts multiple flags (`is_recurring`, `apply_discount`, `is_refund`) that alter its behavior and lead to multiple unrelated tasks being handled within the same function.

File: {StandaloneFunctions.py}
Detected: {send_notification}
Issue: The function uses three flags (`is_email`, `is_sms`, `is_push`) to control its behavior, resulting in multiple execution paths that could be split into separate, more focused functions.

File: {StandaloneFunctions.py}
Detected: {handle_shipping}
Issue: The function uses two flags (`is_expedited`, `is_international`) to control its behavior, leading to multiple unrelated tasks being handled within the same function.
2024-12-09 14:34:23,667 - __main__ - INFO - Received task message
2024-12-09 14:34:23,668 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 14:34:23,668 - __main__ - INFO - Task type: detection
2024-12-09 14:34:23,668 - __main__ - INFO - Task job: middle_man
2024-12-09 14:34:23,670 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 14:34:23,670 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 14:34:36,226 - llm_processor - INFO - Processed result: File:{OrderManager.py}
Detected:{OrderManager}

The `OrderManager` class serves as an intermediary between the `InventoryManager` class and the user interface, primarily delegating its work to the `InventoryManager` class. 

Detected:{OrderManager.place_order_redundant}

The method `place_order_redundant` of the `OrderManager` class also solely delegates its work to the `InventoryManager` class by calling the `check_stock` method, which indicates redundancy.

File:{CustomerManager.py}
Detected:{CustomerManager.notify_customer_direct_delegate}
2024-12-09 14:35:12,318 - __main__ - INFO - Received task message
2024-12-09 14:35:12,319 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 14:35:12,319 - __main__ - INFO - Task type: detection
2024-12-09 14:35:12,320 - __main__ - INFO - Task job: switch_statement_abuser
2024-12-09 14:35:12,321 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 14:35:12,322 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 14:35:16,923 - llm_processor - INFO - Processed result: File: PaymentProcessor.py
Detected: PaymentProcessor.PaymentProcessor.process_payment
Detected: process_refund

File: OrderHandler.py
Detected: OrderHandler.OrderHandler.handle_order
Detected: process_shipping
2024-12-09 14:35:44,680 - __main__ - INFO - Received task message
2024-12-09 14:35:44,680 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 14:35:44,681 - __main__ - INFO - Task type: detection
2024-12-09 14:35:44,681 - __main__ - INFO - Task job: excessive_flags
2024-12-09 14:35:44,683 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 14:35:44,683 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 14:35:44,911 - llm_processor - INFO - Processed result: File: {OrderProcessor.py}
Detected: {OrderProcessor.OrderProcessor.process_order}
Issue: The function takes multiple flags (`is_priority`, `is_gift`, `requires_signature`) that significantly alter its behavior, leading to multiple unrelated tasks being handled within the same function.

File: {OrderProcessor.py}
Detected: {process_refund}
Issue: The function uses two flags (`is_partial`, `is_expedited`) to control its behavior, resulting in distinct execution paths that could be split into separate, more focused functions.

File: {PaymentHandler.py}
Detected: {PaymentHandler.PaymentHandler.handle_payment}
Issue: The function accepts multiple flags (`is_recurring`, `apply_discount`, `is_refund`) that alter its behavior and lead to multiple unrelated tasks being handled within the same function.

File: {StandaloneFunctions.py}
Detected: {send_notification}
Issue: The function uses three flags (`is_email`, `is_sms`, `is_push`) to control its behavior, resulting in multiple execution paths that could be split into separate, more focused functions.

File: {StandaloneFunctions.py}
Detected: {handle_shipping}
Issue: The function uses two flags (`is_expedited`, `is_international`) to control its behavior, leading to multiple unrelated tasks being handled within the same function.
2024-12-09 14:35:53,139 - __main__ - INFO - Received task message
2024-12-09 14:35:53,139 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 14:35:53,140 - __main__ - INFO - Task type: detection
2024-12-09 14:35:53,140 - __main__ - INFO - Task job: inappropriate_intimacy
2024-12-09 14:35:53,142 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 14:35:53,142 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 14:35:53,378 - llm_processor - INFO - Processed result: File:{OrderProcessor.py}
Detected:{OrderProcessor.process_order_direct_access}
Issue: The method directly accesses and modifies private attribute `_stock` of the `Inventory` class, bypassing its public interface and tightly coupling the two classes.

File:{CustomerAndNotification.py}
Detected:{NotificationService.send_direct_access}
Issue: The method directly accesses private attributes `name` and `email` of the `Customer` class, bypassing its public interface and tightly coupling the two classes.
2024-12-09 15:12:31,810 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-09 15:12:31,837 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-09 15:12:31,891 - llm_processor - INFO - Loading model pipeline from Hugging Face API: meta-llama/Llama-3.1-70B-Instruct
2024-12-09 15:12:31,891 - llm_processor - INFO - Model Client loaded from Hugging Face Inference API
2024-12-09 15:12:31,891 - __main__ - INFO - Initializing knowledge base for detection
2024-12-09 15:12:31,892 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-09 15:12:31,892 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-09 15:12:31,894 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-09 15:12:31,894 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-09 15:13:56,250 - __main__ - INFO - Received task message
2024-12-09 15:13:56,251 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 15:13:56,251 - __main__ - INFO - Task type: detection
2024-12-09 15:13:56,252 - __main__ - INFO - Task job: inappropriate_intimacy
2024-12-09 15:13:56,253 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 15:13:56,253 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 15:14:06,823 - llm_processor - INFO - Processed result: File: OrderProcessor.py
Detected: OrderProcessor.process_order_direct_access
Issue: The method directly accesses private attribute `_stock` of the `Inventory` class, bypassing its public interface and tightly coupling the two classes.

File: CustomerAndNotification.py
Detected: NotificationService.send_direct_access
Issue: The method directly accesses private attributes `name` and `email` of the `Customer` class, but in Python these attributes are public, which changes the risk.

Upon reevaluation based on your parameters the revised answer is: 

File: OrderProcessor.py
Detected: OrderProcessor.process_order_direct_access
Issue: The method directly accesses private attribute `_stock` of the `Inventory` class, bypassing its public interface and tightly coupling the two classes.
2024-12-09 15:14:57,767 - __main__ - INFO - Received task message
2024-12-09 15:14:57,768 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 15:14:57,768 - __main__ - INFO - Task type: detection
2024-12-09 15:14:57,768 - __main__ - INFO - Task job: excessive_flags
2024-12-09 15:14:57,770 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 15:14:57,770 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 15:15:28,198 - llm_processor - INFO - Processed result: File: OrderProcessor.py
Detected: OrderProcessor.OrderProcessor.process_order
Issue: The method takes three flags (`is_priority`, `is_gift`, `requires_signature`) that significantly alter its behavior, making it harder to understand and maintain.

File: OrderProcessor.py
Detected: process_refund
Issue: The function uses two flags (`is_partial`, `is_expedited`) to handle different refund scenarios, leading to distinct behaviors within the same function.

File: PaymentHandler.py
Detected: PaymentHandler.PaymentHandler.handle_payment
Issue: The method relies on three flags (`is_recurring`, `apply_discount`, `is_refund`) to handle different payment scenarios, making the function prone to errors and harder to test.

File: StandaloneFunctions.py
Detected: send_notification
Issue: The function takes three flags (`is_email`, `is_sms`, `is_push`) to send different types of notifications, resulting in a function that handles multiple tasks and becomes harder to understand.

File: StandaloneFunctions.py
Detected: handle_shipping
Issue: The function uses two flags (`is_expedited`, `is_international`) to handle different shipping scenarios, leading to multiple behaviors within the same function.
2024-12-09 15:15:45,796 - __main__ - INFO - Received task message
2024-12-09 15:15:45,796 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 15:15:45,797 - __main__ - INFO - Task type: detection
2024-12-09 15:15:45,797 - __main__ - INFO - Task job: switch_statement_abuser
2024-12-09 15:15:45,799 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 15:15:45,799 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 15:16:00,207 - llm_processor - INFO - Processed result: File: {PaymentProcessor.py}
Detected: {PaymentProcessor.PaymentProcessor.process_payment}
Issue: Tightly coupled logic handling multiple distinct payment types, leading to a large conditional structure with four cases, making it hard to extend or maintain.

File: {PaymentProcessor.py}
Detected: {process_refund}
Issue: Overly complex function with four distinct refund cases, each performing non-encapsulated tasks, resulting in a heavy reliance on conditionals and difficulties in testing or extending the function.

File: {OrderHandler.py}
Detected: {OrderHandler.OrderHandler.handle_order}
Issue: Function handling multiple order types with tightly coupled logic across three distinct cases, making it challenging to maintain, extend, or test due to its complexity.

File: {OrderHandler.py}
Detected: {process_shipping}
Issue: Excessive use of conditionals to differentiate between shipping types, leading to a function with three distinct but non-modularized cases, resulting in tightly coupled logic.
2024-12-09 16:00:31,084 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-09 16:00:31,112 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-09 16:00:31,166 - llm_processor - INFO - Loading model pipeline from Hugging Face API: meta-llama/Llama-3.1-70B-Instruct
2024-12-09 16:00:31,167 - llm_processor - INFO - Model Client loaded from Hugging Face Inference API
2024-12-09 16:00:31,167 - __main__ - INFO - Initializing knowledge base for detection
2024-12-09 16:00:31,167 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-09 16:00:31,168 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-09 16:00:31,169 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-09 16:00:31,169 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-09 16:00:37,551 - __main__ - INFO - Received task message
2024-12-09 16:00:37,551 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 16:00:37,552 - __main__ - INFO - Task type: detection
2024-12-09 16:00:37,552 - __main__ - INFO - Task job: long_function
2024-12-09 16:00:37,554 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 16:00:37,554 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 16:00:43,639 - llm_processor - INFO - Processed result: File:{928d61.py}
Detected:OrderProcessor.process_order
Issue:Performs multiple tasks like inventory validation, payment processing, and notification sending, making it long and prone to complexity.
 
File:{b701f4.py}
Detected:process_user_data
Issue:Exceeds a reasonable line threshold, performs multiple tasks like data validation, database storage, and email sending, involving excessive file operations and database interactions.
2024-12-09 20:57:42,940 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-09 20:57:42,970 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-09 20:57:43,017 - llm_processor - INFO - Loading model pipeline from Hugging Face API: meta-llama/Llama-3.1-70B-Instruct
2024-12-09 20:57:43,018 - llm_processor - INFO - Model Client loaded from Hugging Face Inference API
2024-12-09 20:57:43,018 - __main__ - INFO - Initializing knowledge base for detection
2024-12-09 20:57:43,018 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-09 20:57:43,018 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-09 20:57:43,025 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-09 20:57:43,026 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-09 20:58:07,947 - __main__ - INFO - Received task message
2024-12-09 20:58:07,947 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 20:58:07,947 - __main__ - INFO - Task type: detection
2024-12-09 20:58:07,948 - __main__ - INFO - Task job: god_object
2024-12-09 20:58:07,953 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 20:58:07,953 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 20:58:12,030 - llm_processor - INFO - Processed result: File:{a276f9.py}
Detected:GaTectorDataset
Issue:The class has multiple unrelated responsibilities, including data loading, data transformation, data augmentation, and data preprocessing, making it a large class that can be hard to maintain and understand.
2024-12-09 20:58:31,220 - __main__ - INFO - Received task message
2024-12-09 20:58:31,221 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 20:58:31,221 - __main__ - INFO - Task type: detection
2024-12-09 20:58:31,222 - __main__ - INFO - Task job: god_object
2024-12-09 20:58:31,223 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 20:58:31,223 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 20:58:31,601 - llm_processor - INFO - Processed result: None
2024-12-09 20:58:58,404 - __main__ - INFO - Received task message
2024-12-09 20:58:58,405 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 20:58:58,405 - __main__ - INFO - Task type: detection
2024-12-09 20:58:58,405 - __main__ - INFO - Task job: god_object
2024-12-09 20:58:58,417 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 20:58:58,417 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 20:59:01,547 - llm_processor - INFO - Processed result: File:{3fd860.py}
Detected:GodClass
Issue:The class takes on multiple unrelated responsibilities such as configuration, database connection, logging, user interaction, and data processing, making it non-cohesive and hard to maintain.
2024-12-09 20:59:32,746 - __main__ - INFO - Received task message
2024-12-09 20:59:32,747 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 20:59:32,748 - __main__ - INFO - Task type: detection
2024-12-09 20:59:32,748 - __main__ - INFO - Task job: god_object
2024-12-09 20:59:32,755 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 20:59:32,755 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 20:59:41,235 - llm_processor - INFO - Processed result: File:{3fd860.py}
Detected:GodClass
Issue:The GodClass takes on multiple unrelated responsibilities, including configuration, database connection, user authentication, logging, data caching, and user interface input, making it a large class (God Object) code smell.

File:{3fd860.py}
Detected:GodClass
Issue:The GodClass class has 14 methods, many of which are not related to a single domain concern, making it difficult to maintain or understand.

File:{a276f9.py}
Detected:GaTectorDataset
Issue:The GaTectorDataset class has many methods and handles multiple responsibilities, including data loading, transformation, and processing, but it is a relatively domain-specific class with a clear purpose, so it is cohesive and not considered a large class code smell.
2024-12-09 21:06:20,520 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-09 21:06:20,528 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-09 21:06:20,573 - llm_processor - INFO - Loading model pipeline from Hugging Face API: meta-llama/Llama-3.1-70B-Instruct
2024-12-09 21:06:20,574 - llm_processor - INFO - Model Client loaded from Hugging Face Inference API
2024-12-09 21:06:20,574 - __main__ - INFO - Initializing knowledge base for detection
2024-12-09 21:06:20,574 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-09 21:06:20,574 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-09 21:06:20,575 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-09 21:06:20,575 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-09 21:07:20,218 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-09 21:07:20,244 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-09 21:07:20,293 - llm_processor - INFO - Loading model pipeline from Hugging Face API: meta-llama/Llama-3.1-70B-Instruct
2024-12-09 21:07:20,293 - llm_processor - INFO - Model Client loaded from Hugging Face Inference API
2024-12-09 21:07:20,294 - __main__ - INFO - Initializing knowledge base for detection
2024-12-09 21:07:20,294 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-09 21:07:20,294 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-09 21:07:20,296 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-09 21:07:20,296 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-09 21:07:52,361 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-09 21:07:52,390 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-09 21:07:52,443 - llm_processor - INFO - Loading model pipeline from Hugging Face API: meta-llama/Llama-3.1-70B-Instruct
2024-12-09 21:07:52,443 - llm_processor - INFO - Model Client loaded from Hugging Face Inference API
2024-12-09 21:07:52,444 - __main__ - INFO - Initializing knowledge base for detection
2024-12-09 21:07:52,444 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-09 21:07:52,445 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-09 21:07:52,445 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-09 21:07:52,446 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-09 21:08:05,697 - __main__ - INFO - Received task message
2024-12-09 21:08:05,697 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 21:08:05,698 - __main__ - INFO - Task type: detection
2024-12-09 21:08:05,698 - __main__ - INFO - Task job: god_object
2024-12-09 21:08:05,698 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 21:08:05,699 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 21:08:06,507 - llm_processor - INFO - Processed result: None
2024-12-09 21:08:06,509 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 21:08:06,510 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 21:08:11,710 - llm_processor - INFO - Processed result: File:3fd860.py
Detected:GodClass
Issue:The class takes on multiple unrelated responsibilities such as configuration management, database connections, logging, user authentication, data caching, and user interaction, violating the Single Responsibility Principle and making it a 'God Object' class.

Note: The other provided class, 'StringUtils', does not qualify as a large class (God Object) as it is a cohesive, single-purpose utility class focusing on string manipulation.
2024-12-09 21:08:11,715 - __main__ - ERROR - Error generating response for task message with correlation ID: 1234
2024-12-09 21:08:11,716 - __main__ - ERROR - 'str' object is not a mapping
2024-12-09 21:09:38,639 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-09 21:09:38,661 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-09 21:09:38,714 - llm_processor - INFO - Loading model pipeline from Hugging Face API: meta-llama/Llama-3.1-70B-Instruct
2024-12-09 21:09:38,714 - llm_processor - INFO - Model Client loaded from Hugging Face Inference API
2024-12-09 21:09:38,715 - __main__ - INFO - Initializing knowledge base for detection
2024-12-09 21:09:38,715 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-09 21:09:38,715 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-09 21:09:38,716 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-09 21:09:38,716 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-09 21:09:44,877 - __main__ - INFO - Received task message
2024-12-09 21:09:44,877 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 21:09:44,878 - __main__ - INFO - Task type: detection
2024-12-09 21:09:44,878 - __main__ - INFO - Task job: god_object
2024-12-09 21:09:44,879 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 21:09:44,879 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 21:09:45,683 - llm_processor - INFO - Processed result: None
2024-12-09 21:09:45,686 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 21:09:45,687 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 21:09:45,925 - llm_processor - INFO - Processed result: File:3fd860.py
Detected:GodClass
Issue:The class takes on multiple unrelated responsibilities such as configuration management, database connections, logging, user authentication, data caching, and user interaction, violating the Single Responsibility Principle and making it a 'God Object' class.

Note: The other provided class, 'StringUtils', does not qualify as a large class (God Object) as it is a cohesive, single-purpose utility class focusing on string manipulation.
2024-12-09 21:09:45,929 - __main__ - ERROR - Error generating response for task message with correlation ID: 1234
2024-12-09 21:09:45,930 - __main__ - ERROR - '3fd860.py'
2024-12-09 21:10:57,313 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-09 21:10:57,330 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-09 21:10:57,384 - llm_processor - INFO - Loading model pipeline from Hugging Face API: meta-llama/Llama-3.1-70B-Instruct
2024-12-09 21:10:57,384 - llm_processor - INFO - Model Client loaded from Hugging Face Inference API
2024-12-09 21:10:57,385 - __main__ - INFO - Initializing knowledge base for detection
2024-12-09 21:10:57,385 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-09 21:10:57,385 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-09 21:10:57,387 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-09 21:10:57,387 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-09 21:11:02,360 - __main__ - INFO - Received task message
2024-12-09 21:11:02,361 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 21:11:02,361 - __main__ - INFO - Task type: detection
2024-12-09 21:11:02,361 - __main__ - INFO - Task job: god_object
2024-12-09 21:11:02,369 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 21:11:02,369 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 21:11:03,334 - llm_processor - INFO - Processed result: File:{3fd860.py}
Detected:GodClass
Issue:The class takes on multiple unrelated responsibilities such as configuration, database connection, logging, user interaction, and data processing, making it non-cohesive and hard to maintain.
2024-12-09 21:12:32,352 - __main__ - INFO - Getting queue URL for queue: LLMTaskQueue
2024-12-09 21:12:32,382 - __main__ - INFO - Getting queue URL for queue: LLMResponseQueue
2024-12-09 21:12:32,432 - llm_processor - INFO - Loading model pipeline from Hugging Face API: meta-llama/Llama-3.1-70B-Instruct
2024-12-09 21:12:32,432 - llm_processor - INFO - Model Client loaded from Hugging Face Inference API
2024-12-09 21:12:32,433 - __main__ - INFO - Initializing knowledge base for detection
2024-12-09 21:12:32,433 - utils.rag.retrieval - INFO - Initializing knowledge base for detection
2024-12-09 21:12:32,433 - utils.rag.retrieval - INFO - Loading knowledge base from database
2024-12-09 21:12:32,434 - utils.rag.retrieval - INFO - Checking for saved embeddings and model
2024-12-09 21:12:32,434 - utils.rag.retrieval - INFO - Loading embeddings and model from file
2024-12-09 21:12:39,502 - __main__ - INFO - Received task message
2024-12-09 21:12:39,503 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 21:12:39,503 - __main__ - INFO - Task type: detection
2024-12-09 21:12:39,503 - __main__ - INFO - Task job: god_object
2024-12-09 21:12:39,511 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 21:12:39,512 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 21:12:40,517 - llm_processor - INFO - Processed result: File:{3fd860.py}
Detected:GodClass
Issue:The class takes on multiple unrelated responsibilities such as configuration, database connection, logging, user interaction, and data processing, making it non-cohesive and hard to maintain.
2024-12-09 21:12:59,664 - __main__ - INFO - Received task message
2024-12-09 21:12:59,665 - __main__ - INFO - Processing task message with correlation ID: 1234
2024-12-09 21:12:59,665 - __main__ - INFO - Task type: detection
2024-12-09 21:12:59,666 - __main__ - INFO - Task job: god_object
2024-12-09 21:12:59,673 - __main__ - INFO - Generating response for task message with correlation ID: 1234
2024-12-09 21:12:59,674 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 21:12:59,910 - llm_processor - INFO - Processed result: File:{3fd860.py}
Detected:GodClass
Issue:The class takes on multiple unrelated responsibilities such as configuration, database connection, logging, user interaction, and data processing, making it non-cohesive and hard to maintain.
2024-12-09 21:25:47,715 - __main__ - INFO - Received task message
2024-12-09 21:25:47,716 - __main__ - INFO - Processing task message with correlation ID: a2cc23e6-f4bd-4995-8109-47bc260b1f41
2024-12-09 21:25:47,717 - __main__ - INFO - Task type: detection
2024-12-09 21:25:47,717 - __main__ - INFO - Task job: god_object
2024-12-09 21:25:47,759 - __main__ - INFO - Generating response for task message with correlation ID: a2cc23e6-f4bd-4995-8109-47bc260b1f41
2024-12-09 21:25:47,760 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 21:25:59,834 - llm_processor - INFO - Processed result: File:{44dafd.py}
Detected:GaTectorBody
Issue:This class appears to take on multiple responsibilities, as it implements both the main logic of the GaTector model and the GOO network, which is a part of the attention mechanism. The class is also quite large and complex, indicating that it may be handling more than one clear domain concern.

File:{9d1b30.py}
Detected:ResNet
Issue:This class, which implements a ResNet architecture, appears to be overly complex and may be taking on too many responsibilities. It not only defines the layers of the ResNet but also implements additional layers specific to the GOO network (e.g., `layer5_scene` and `layer5_face`). This mixing of concerns could make the class harder to maintain and understand.
2024-12-09 21:31:09,125 - __main__ - INFO - Received task message
2024-12-09 21:31:09,126 - __main__ - INFO - Processing task message with correlation ID: 7f14453c-6775-4b5b-bc12-e27d444e3401
2024-12-09 21:31:09,127 - __main__ - INFO - Task type: detection
2024-12-09 21:31:09,127 - __main__ - INFO - Task job: god_object
2024-12-09 21:31:09,172 - __main__ - INFO - Generating response for task message with correlation ID: 7f14453c-6775-4b5b-bc12-e27d444e3401
2024-12-09 21:31:09,173 - llm_processor - INFO - Processing task with LLM model pipeline
2024-12-09 21:31:26,179 - llm_processor - INFO - Processed result: File:{d22b3c.py}
Detected:GaTectorDataset
Issue: This class handles data processing, image loading, image transformations, augmentation, and target preparation, making it a large class that handles multiple responsibilities.

File:{44dafd.py}
Detected:GaTectorBody
Issue: This class handles multiple tasks such as backbone feature extraction, feature map processing, upsampling, detection, and GOO network processing. The class is quite large and has multiple responsibilities.

File:{2dfdd7.py}
Detected:YOLO
Issue: This class handles multiple tasks such as model initialization, image processing, object detection, text drawing, and performance evaluation. It also contains a lot of logic related to the YOLO model and object detection, making it a large and complex class.

File:{6affa8.py}
Detected:YOLOLoss
Issue: This class handles the loss calculation for the YOLO object detection model. While loss calculation might seem like a singular task, the class has grown to handle multiple aspects of object detection such as boundary box adjustments, overlaps, and cross-entropy.
